---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Kathy Hoang and 506333118"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r}
#Clear Environment
rm(list=ls())
```


```{r}
#load libraries
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)
```

## ISL Exercise 9.7.1 (10pts)
This problem involves hyperplanes in two dimensions.
(a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set of
points for which 1 + 3X1 − X2 > 0, as well as the set of points
for which 1 + 3X1 − X2 < 0.

(b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0.
Indicate the set of points for which −2 + X1 + 2X2 > 0, as well
as the set of points for which −2 + X1 + 2X2 < 0.

## ISL Exercise 9.7.2 (10pts)
We have seen that in p = 2 dimensions, a linear decision boundary
takes the form $\beta_0 + \beta_1X_1 + \beta_2X_2 =0$.
We now investigate a non-linear decision boundary.

### a
(a) Sketch the curve $(1 + X_1)^2 + (2 − X_2)^2 = 4:$

### b
(b) On your sketch, indicate the set of points for which

$(1 + X_1)^2 + (2 − X_2)^2 > 4;$

as well as the set of points for which
$(1 + X_1)^2 + (2 − X_2)^2 $\le$ 4:$

### c
(c) Suppose that a classifier assigns an observation to the blue class
if
$(1 + X_1)^2 + (2 − X_2)^2 > 4;$
and to the red class otherwise. To what class is the observation
(0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?

### d
(d) Argue that while the decision boundary in (c) is not linear in
terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$,
and $X_2^2$.



## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

```{r}
#load data
carseats_ds <- Carseats
carseats_ds |> tbl_summary()

#check for null
is.na(carseats_ds) |> colSums()

# Recode Sales as a Factor
carseats_ds$Sales <- factor(ifelse(carseats_ds$Sales > 8, "High", "Low"))
head(carseats_ds,5)
```

```{r}
#Split Data
set.seed(212)

carseats_split <- initial_split(
  carseats_ds, 
  prop = 0.75, 
  strata = Sales)
carseats_split
carseats_train <- training(carseats_split)
carseats_test <- testing(carseats_split)

dim(carseats_train)
dim(carseats_test)
```


```{r}
#Preprocess Data and Recipe
svm_carseats_recipe <- recipe(
  Sales ~ ., 
  data = carseats_train) |>
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

### SVM with linear kernel 

```{r}
svm_linear_mod <- 
  svm_linear(
    mode = "classification",
    cost = tune()
  ) %>% 
  set_engine("kernlab")
svm_linear_mod

#Bundle
svm_linear_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_linear_mod)

#Tune
param_grid_linear <- grid_regular(
  cost(range = c(-2, 3)), #tune cost (C)
  #degree = 1 bc linear
  levels = c(5)
  )
```
```{r}
#Cross-Validation
set.seed(212)

svm_linear_folds <- vfold_cv(carseats_train, v = 5)

#Fit C-V
svm_linear_fit <- svm_linear_wf %>%
  tune_grid(
    resamples = svm_linear_folds,
    grid = param_grid_linear,
    metrics = metric_set(roc_auc, accuracy)
    )

svm_linear_fit
```
```{r}
#Cost
svm_linear_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()
```
```{r}
#Best Model
svm_linear_fit %>%
  show_best("roc_auc")

best_svm_linear <- svm_linear_fit %>%
  select_best("roc_auc")

#Final Model
final_svm_linear <- svm_linear_wf %>%
  finalize_workflow(
    best_svm_linear
  )

#Fit Final Model
final_svm_lin_fit <- final_svm_linear %>%
  fit(data = carseats_train)
final_svm_lin_fit

#Predict
final_svm_preds <- final_svm_lin_fit %>%
  predict(new_data = carseats_test) %>%
  bind_cols(carseats_test)
final_svm_preds
```

### SVM with polynomial kernel 
Tune the degree and regularization parameter $C$.
```{r}
#Model
svm_mod <- 
  svm_poly(
    mode = "classification",
    cost = tune(),
    degree = tune(),
    # scale_factor = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod

#Bundle
svm_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_mod)
svm_wf

#Tune
param_grid <- grid_regular(
  cost(range = c(-2,3)), #tune cost (C) 
  #cost(range = c(-3,3)), #tune cost (C) 
  #on a logarithmic scale
  #10-3 to 10^3 (0.0001, 0.01, 0.1, 1, 100, 1000)
  degree(range = c(1, 5)), #tune degrees 1-5
  #scale_factor(range = c(-1, 1)),
  levels = c(5)
  )
param_grid

#Cross-Valisdation
set.seed(212)
svm_folds <- vfold_cv(carseats_train, v = 5)
svm_folds

#Fit C-V 
svm_fit <- svm_wf %>%
  tune_grid(
    resamples = svm_folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

```{r}
#Cost
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

#Degree
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = degree, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Degree", y = "CV AUC") +
  scale_x_log10()
```

```{r}
#Additional Plots - (better graphs mentioned in OH)

#Degree on x-axis, grouped by cost
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = degree, y = mean, alpha = cost)) +
  geom_point() +
  geom_line(aes(group = cost)) +
  labs(x = "Degree", y = "CV AUC") +
  scale_x_log10() 

#Cost on x-axis, grouped by degree
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = cost, y = mean, alpha = degree)) +
  geom_point() +
  geom_line(aes(group = degree)) +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10() 
```

```{r}
#Best Model
svm_fit %>%
  show_best("roc_auc")

best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm

#Final Model
final_svm <- svm_wf %>%
  finalize_workflow(
    best_svm
  )
final_svm

#Fit Final Model
final_svm_fit <- final_svm %>%
  fit(data = carseats_train)
final_svm_fit

#Predict
final_svm_preds <- final_svm_fit %>%
  predict(new_data = carseats_test) %>%
  bind_cols(carseats_test)
final_svm_preds
```

### SVM with radial kernel
```{r}
svm_radial_mod <- 
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>% 
  set_engine("kernlab")
```

```{r}
#Bundle
svm_radial_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_radial_mod)
```

```{r}
#Tune
param_grid_radial <- grid_regular(
  #cost(range = c(-2, 2)), #tune cost (C)
  cost(range = c(-3,5)), #tune cost (C)
  rbf_sigma(range = c(-4, -2)), #tune scale
  levels = c(5)
  )
```

```{r}
#Cross-Validation
set.seed(212)

svm_radial_folds <- vfold_cv(carseats_train, v = 5)

#Fit C-V
svm_radial_fit <- svm_radial_wf %>%
  tune_grid(
    resamples = svm_radial_folds,
    grid = param_grid_radial,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_radial_fit

svm_radial_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean, color = as.factor(rbf_sigma))) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

```
### HW4 Comparisons

Sources:

Intuitive explanation on Tuning Cost and Gamma Parameters <https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py>

The degree parameter controls the flexibility of the decision boundary. Higher degree kernels yield a more flexible decision boundary. <https://stats.stackexchange.com/questions/348318/degree-parameter-for-svm-polynomial-kernel>

```{r}
## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 
