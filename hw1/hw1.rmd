---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 23, 2024 @ 11:59PM (Extension to Feb 6, 2024)"
author: "Kathy Hoang UID:506333118"
date: "`r format(Sys.time(), '%d %B, %Y')`"


header-includes:
- \usepackage{amsthm}
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10pts)

Consider the regression model
$$
Y = f(X) + \epsilon,
$$
where $\operatorname{E}(\epsilon) = 0$. 

### Optimal regression function

Show that the choice
$$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$
minimizes the mean squared prediction error
$$
\operatorname{E}\{[Y - f(X)]^2\},
$$
where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)

#### Optimal Regression Function ANSWER 


$$
\begin{align*}
& \operatorname{E}\{[Y - f(X)]^2\} = 
\operatorname{E}\{\underbrace{(Y- \operatorname{E}(Y | X)}_\text{a} + \underbrace{\operatorname{E}(Y | X) - f(X)}_\text{b})^2\} \\
& \text{Recall } (a+b)^2 = a^2 + 2ab + b^2 \\
&= \operatorname{E}[(Y-\operatorname{E}(Y | X))^2] 
+ 2\underbrace{\operatorname{E}[(Y-\operatorname{E}(Y | X))] \operatorname{E}[(\operatorname{E}(Y | X)- f(X))}_\text{zero in on this below}
+ \operatorname{E}[(\operatorname{E}(Y | X)- f(X))^2] 
\\
\text{by the law of iterated expectations } \operatorname{E}[X]&= \operatorname{E}[\operatorname{E}[Y | X] \\
&= \operatorname{E}[\operatorname{E}[Y-\operatorname{E}(Y | X)] [\operatorname{E}(Y | X) - f(X) | X] \\
&= \operatorname{E}[\underbrace{(\operatorname{E}[(Y|X)-\operatorname{E}(Y | X))}_\text{0} (\operatorname{E}(Y | X) - f(X))] \\
&= 0 \\

\\
\operatorname{E}\{[Y - f(X)]^2\} &= \underbrace{\operatorname{E}([Y - \operatorname{E}[(Y|X)]^2)}_\text{greater or equal to 0} + \underbrace{\operatorname{E}[\operatorname{E}(Y|X) - f(X)]^2}_\text{greater or equal to 0} \\
\operatorname{E}(Y | X) - f(X) &= 0 \\
f(x) &= \operatorname{E}(Y | X) \\
& \text{Recall } \underbrace{(a-b)^2}_\text{greater or equal to 0} +  \underbrace{(c-d)^2}_\text{minimized when c = d} = 0 \\

& \text{Thus, } 

f_{\text{opt}}(X) = \operatorname{E}(Y | X) \text{ minimizes } \operatorname{E}\{[Y - f(X)]^2\}.  \\

\end{align*}
$$

### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$
where the expectation averages over the variability in $y_0$ and $\hat f$.

#### Bias-Variance Trade-off Answer 

$$
\text{Given that} \:
Y = f(X) + \epsilon \
\text{and} \
\operatorname{E}(\epsilon) = 0 
\text{,}
\\ \text{we can decompose the expected mean squared test error at } X_{0} 
\text{ into irreducible (1) and reducible error (2). }
\\
$$
$$
\begin{align*}

\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} 
&= \operatorname{E}\{[y_0 - f(x_0) + f(x_0) - \hat f(x_0)]^2\}

\\ &= \operatorname{E}\{[y_0 - f(x_0)]^2\} +
\operatorname{E}\{[ f(x_0) - \hat f(x_0)]^2\} -
2\operatorname{E}\{[(f(x_0) - \hat f(x_0))(y_0 -  f(x_0))]\} \\

& \text{Recall } (a-b)^2 = a^2 - 2ab + b^2 \\
a^2 &= \operatorname{E}\{[y_0 - f(x_0)]^2\} \\
b^2 &= \operatorname{E}\{[ f(x_0) - \hat f(x_0)]^2\} \\
2ab &= 2\operatorname{E}\{[(f(x_0) - \hat f(x_0))(y_0 -  f(x_0))]\} \\
&= 2\operatorname{E}\{[f(x_0)(y_0)-f(x_0)^2- \hat f(x_0)(y_0) + f(x_0) \hat f(x_0)]\}\\
&= 2(f(x_0)^2-f(x_0)^2-f(x_0)\operatorname{E}\{[\hat f(x_0)]\}+f(x_0)\operatorname{E}\{[\hat f(x_0)]\}) \\
&= 2(0) \\
&=0 \\


\\ &= \operatorname{E}\{[y_0 - f(x_0)]^2\} +
\operatorname{E}\{[ f(x_0) - \hat f(x_0)]^2\} \\

& \text{For simplicity and clarity, I will break these terms up to solve individually and combine them in the end.}
\end{align*}
$$
$$
\begin{align*}
& \text{(1) reducible error} \\
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} &= 
\operatorname{E}\{[\hat f(x_0) + \epsilon - \hat f(x_0)]^2\} \text{ sub in } \hat f(x_0) + \epsilon \text{ for } y_0 \\
&= \operatorname{E}(\epsilon^2) -  \operatorname{E}(\epsilon)^2 + \operatorname{E}(\epsilon)^2 \\ &= E[\epsilon^2] - E[\epsilon]^2 + 0 \\
&= \operatorname{Var}(\epsilon)
\end{align*}
$$
$$
\begin{align*}
& \text{(2) irreducible error} \\

\operatorname{E}\{[ f(x_0) - \hat f(x_0)]^2\}
&= \operatorname{E}\{[f(x_0) + {E}[\hat f(x_0)] - {E}[\hat f(x_0) ] - \hat f(x_0)]^2]\} \\
&= \operatorname{E}\{[f(x_0) + {E}[\hat f(x_0)]]^2\} 
- 2\operatorname{E}\{[ (f(x_0) + E[\hat f(x_0)]) * (E[\hat f(x_0) ] - \hat f(x_0)])]\}
+ \operatorname{E}\{[\hat f(x_0) + {E}[\hat f(x_0)]^2]\} \\

& \text{Recall } (a-b)^2 = a^2 - 2ab + b^2 \text{, we will zero in on the latter part b in 2ab}. \\
& 2\operatorname{E}\{[ (f(x_0) + E[\hat f(x_0)]) * (E[\hat f(x_0) ] - \hat f(x_0)])]\} \\
& \operatorname{E}\{[\hat f(x_0) - {E}\{[(\hat f(x_0))]\} \\
&= E[\hat f(x_0)] - E[\hat f(x_0)] \text{    because the expected value of an expected value is just itself} \\
&= 0 \\

& \text{Thus, we are left with } \\

&= \operatorname{E}\{[f(x_0) + {E}[\hat f(x_0)]]^2\} 
+ \operatorname{E}\{[\hat f(x_0) + {E}[\hat f(x_0)]^2]\} \\
&= \operatorname{Var}(\hat f(x_0))  + [\operatorname{Bias}(\hat f(x_0))]^2
\end{align*}
$$
$$
\begin{align*}
& \text{Combining (1) reducible error and (2) irreducible error we get a bias-variance decomposition of} \\
&= \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2 + \operatorname{Var}(\epsilon)

\end{align*}
$$

## ISL Exercise 2.4.3 (10pts)

```{r, eval = F}
library(tidyverse)
fit <- lm(sales ~ TV, data = )
```

## ISL Exercise 2.4.4 (10pts)



## ISL Exercise 2.4.10 (30pts)

Your can read in the `boston` data set directly from url <https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>. A documentation of the `boston` data set is [here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

::: {.panel-tabset}

#### R

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

#### Python

```{python}
import pandas as pd
import io
import requests

url = "https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/Boston.csv"
s = requests.get(url).content
Boston = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col = 0)
Boston
```
:::

```{r part a load dataset}
# install.packages("ISLR2")
library(ISLR2)
library(GGally)
Boston
?Boston
dim(Boston)
```

##### a. Boston Dataset Dimensions
There are 506 rows and 13 columns in the Boston data set. The rows represent the 506 suburbs in Boston and their housing values. Each row corresponds to one suburb in Boston and the predictor variables for that suburb. Each column represents a different predictor variable for the housing values (Ex. crim= per capita crime rate by town, zn = proportion of residential land zoned for lots over 25,000 sq.ft, etc).

More information about the Boston data set can be found from this source: https://rdrr.io/cran/ISLR2/man/Boston.html

##### b. Pairwise Scatterplots
```{r part b scatterplots, evalue = T}
Boston %>% ggpairs(lower = list(continuous = wrap("points", alpha = 0.3, size=0.5)), diag=list(continuous='barDiag'),
                   upper = list(continuous = wrap("cor", size = 2))) + theme(axis.text = element_text(size = 5), #change axis label sizes to stop overlapping 
      strip.text.x = element_text(size = 5), #change row text label sizes
      strip.text.y = element_text(size = 5)) #change col text label sizes
```


Findings: Some of the variables appear to be correlated. For example, tax (full-value property-tax rate per $10,000) and rad (index of accessibility to radial highways) have a strong positive correlation coefficient of 0.91. Indus (proportion of non-retail business acres per town) and nox (nitrogen oxides concentration (parts per 10 million)) also have a high correlation coefficient of 0.764, which suggests that the increasing presence of non-retail businesses may correspond to increases in the nitrogen oxides concentration levels. Lstat (lower status of the population (percent)) and medv (median value of owner-occupied homes in 1000s of dollars) have a negative correlation, with a correlation coefficient of -0.738, which is resonable since more people with lower status would be expected to have lower median values in homes.

The diagonal bar graphs show that few of the variables have a normal distribtuion, such as rm(average number of rooms per dwellin) and medv(median value of owner-occupied homes in $1000s). Variables like age (proportion of owner-occupied units built prior to 1940) are skewed to the left, which implies that most of the units are old and built before 1940 and there are very few cases of units built after 1940. Other variables dis (weighted mean of distances to five Boston employment centres) and lstat (lower status of the population (percent)) are skewed to the right. We can interpret this as most units are in the lower status of the population and most units have a short average distance to nearby employment centers in Boston.

##### c.Association of Predictors with Per Capita Crime Rate
There does seem to be an association between per capita crime rate and some of the predictor variables, though the variables with the highest association are considered moderately correlated (between 0.5 and 0.7). Most of the predictor variables are weakly correlated with per capita crime rate.

| rad&crime = 0.626 The higher the index of accessibility to radial highways, the higher per capita crime rate by town.
| tax&crime = 0.583 The higher the full-value property-tax rate per $10,000, the higher per capita crime rate by town.
| lstat&crime = 0.456 More crime happens for those in lower status of the population.
| medv&crime = -3.888 The higher the median value of owner-occupied homes, the less crime that occurs.


##### d. Census Tracts and Per Capita Crime Rate
Do the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
```{r}
hist(Boston$crim, main = "Distribution of Crime Rates in Boston Suburbs", xlab = "Crime Rate per Capita by Town", col = "lightblue")
summary(Boston$crim)
```
The distribution of crime rates is right-skewed, with most of the data points falling below 10. The per capita crime rate ranges from 0.00632 to 88.9762, with a median of 0.25651 and mean of 3.61352. While there are some census tracts with particularly high crime rates with the maximum being 88.97, the majority of the census tracts have low crime rates. 

```{r}
hist(Boston$tax, main = "Distribution of Property Tax Rates in Boston Suburbs", xlab = " Property Tax Rates (per $10,000)", col = "lightblue")
summary(Boston$tax)
```
The tax rate (per 10000 dollars) ranges from 187 to 711 dollars, with a median of 330 dollars and mean of 408.2 dollars. Since the mean is greater than the median, the distribution of property tax rates is right-skewed, and shows that most of the data points fall below 400 dollars. There are several census tracts with particularly high tax rates over 650 dollars, as demonstrated by the histogram and the Upper Quartile being 666 dollars. 


```{r}
hist(as.numeric(Boston$ptratio), main = "Distribution of Pupil-Teacher Ratio in Boston Suburbs", xlab = "Pupil-Teacher Ratio", col = "lightblue")
summary(as.numeric(Boston$ptratio))
```

The pupil-teacher ratio ranges from 12.6 to 22, with a median of 19.05 and mean of 18.46. The mean is less than the median, so the distribution is left-skewed. The histogram shows that some of the census tracts relatively have high pupil-teacher ratios with the frequency peaking around 20.

##### e. Census Tracts and Charles River
```{r}
sum(Boston$chas == 1)
```
35 of the census tracts bound the Charles river.

##### f. Median Value of Owner-Occupied Homes and Census Tracts
```{r}
median(Boston$ptratio)
```
The median pupil-teacher ratio among the towns is 19.05.

##### g. Cennsus Tracts with Low Median Value of Homes
(g) Which census tract of Boston has lowest median value of owner- occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r}
#Find suburb numbers with lowest median value of owner-occupied homes
which(Boston$medv == min(Boston$medv))

#Find the values of the other predictors for that census tract
Boston %>% filter(medv == min(medv)) 

summary(Boston$zn)
summary(Boston$indus)
summary(Boston$chas)
summary(Boston$nox)
summary(Boston$rm)
summary(Boston$age)
summary(Boston$dis)
summary(Boston$rad)
summary(Boston$lstat)

```
There are two census tracts with the lowest median value of 5: 399 and 406. The values of the other predictors for that census tract are outputted in the table above, with the first row representing census tract 399 and the second row corresponding to census tract 406.

The values of the other predictors for census tract 399 are: crim = 38.3518, zn = 0, indus = 18.1, chas = 0, nox = 0.693, rm = 5.453, age = 100, dis = 1.4896, rad = 24, tax = 666, ptratio = 20.2, and lstat = 30.59. 

The values of the other predictors for census tract 406 are: crim = 67.9208, zn = 0, indus = 18.1, chas = 0, nox = 0.693, rm = 5.683, age = 100, dis = 1.4254, rad = 24, tax = 666, ptratio = 20.2, and lstat = 22.98. 

Several of the other predictors for both census tracts have values at the upper end of the range or are the maximum values for the predictors, such as the proportion of non-retail business acres per town (indus), the index of accessibility to radial highways (rad), the full-value property-tax rate per $10,000 (tax), and the percentage of owner-occupied units built prior to 1940 (age). The percentage of the lower status of the population (lstat) is 30.59 for census tract 399 and 20.34 for census tract 406, which is at also on the higher end of the range for that predictor. Two of the predictors, zn and chas, have the lowest possible value of 0 for both census tracts.

##### h. Cennsus Tracts and Rooms per Dwelling
 In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.

```{r}
sum(Boston$rm > 7)
sum(Boston$rm > 8)

more_than_eight = Boston %>% filter (Boston$rm > 8)
more_than_eight
summary(more_than_eight)

hist(more_than_eight$medv, main = "Distribution of Median Value of Owner-Occupied Homes \n in Boston Suburbs with More than 8 Rooms per Dwelling", xlab = "Median Value of Owner-Occupied Homes in $1000s", col = "lightblue")

hist(more_than_eight$zn, main = "Distribution of Proportion of Residential Land Zoned for Lots Over 25,000 sq.ft \n in Boston Suburbs with More than 8 Rooms per Dwelling", xlab = "Proportion of Residential Land Zoned for Lots Over 25,000 sq.ft", col = "lightblue")

hist(more_than_eight$lstat, main = "Distribution of Lower Status of the Population \n in Boston Suburbs with More than 8 Rooms per Dwelling", xlab = "Lower Status of the Population (percent)", col = "lightblue")


hist(more_than_eight$rad, main = "Distribution of Index of Accessibility to Radial Highways \n in Boston Suburbs with More than 8 Rooms per Dwelling", xlab = "Index of Accessibility to Radial Highways", col = "lightblue")

hist(more_than_eight$age, main = "Distribution of Proportion of Owner-Occupied Units Built Prior to 1940 \n in Boston Suburbs with More than 8 Rooms per Dwelling", xlab = "Proportion of Owner-Occupied Units Built Prior to 1940", col = "lightblue")


```
The number of census tracts that average more than seven rooms per dwelling is 64. 
The number of census tracts that average more than eight rooms per dwelling is 13. 

The census tracts that average more than eight rooms per dwelling are likely to have high median values of owner-occupied homes (medv), low proportion of residential land zoned for lots over 25,000 sq.ft (zn), and low percentages of the lower status of the population (lstat). They are also likely to low index of accessibility to radial highways (rad) and high percentages of owner-occupied units built prior to 1940 (age).

## ISL Exercise 3.7.3 (12pts)
3. Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High School), X4 = Interac- tion between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50,βˆ1 = 20,βˆ2 = 0.07,βˆ3 = 35,βˆ4 = 0.01,βˆ5 = −10.

##### a. Which answer is correct and why? 

True - iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough. 

Y = 50 + 20(GPA)+ 0.07(IQ) + 35(Level) + 0.01(GPA * IQ) - 10(GPA * Level)
Let X1 = GPA, X2 = IQ, Level = 1 for College and 0 for High School
Y = 50 + 20(X1)+ 0.07(X2) + 35(Level) + 0.01(X1 * X2) - 10(X1 * Level)

For High School (Level = 0):
Y = 50 + 20(X1)+ 0.07(X2) + 35(0) + 0.01(X1 * X2) - 10(X1 * 0) 
Y = 50 + 20(X1)+ 0.07(X2) + 0.01(X1 * X2)

For College (Level = 1):
Y = 50 + 20(X1)+ 0.07(X2) + 35(1) + 0.01(X1 * X2) - (10)(X1)*1)
Y = 85 + 20(X1)+ 0.07(X2) + 0.01(X1 * X2) - 10(X1)

Once GPA is high enough, then -10(X1*Level),the interaction between GPA and level, will be more negative for college students, which means that high school graduates earn more, on average, than college graduates.


##### b. Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.

Let IQ = 110, GPA =4.0, and Level = 1 for College
Y = 50 + 20(4.0)+ 0.07(110) + 35(1) + 0.01(4.0 * 110) - 10(4.0 * 1)
Y = 50 + 80 + 7.7 + 35 + 4.4 - 40
Y = 137.1

The salary of a college graduate with IQ of 110 and a GPA of 4.0 is expected to be 137.1 thousand dollars.

##### c. True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

False - Having a small coefficient for the GPA/IQ interaction term does not necessarily mean that there is very little evidence of an interaction effect. The statistical signfiicance of an interaction effect is determined by the p-value of the interaction term, not the size of the coefficient. In other words, even if the coefficient is very small, there can still be a lot of evidence of an interaction effect if the p-value tells us that the interaction term is statistically significant.

## ISL Exercise 3.7.15 (20pts)

##### a. For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

```{r}
boston.zn <- lm(crim ~ zn, data = Boston)
summary(boston.zn)
#plot(boston.zn)
```

```{r}
boston.indus <- lm(crim ~ indus, data = Boston)
summary(boston.indus)
#plot(boston.indus)
```

```{r}
boston.chas <- lm(crim ~ chas, data = Boston)
summary(boston.chas)
#plot(boston.chas)
```

```{r}
boston.nox <- lm(crim ~ nox, data = Boston)
summary(boston.nox)
#plot(boston.nox)
```

```{r}
boston.rm <- lm(crim ~ rm, data = Boston)
summary(boston.rm)
#plot(boston.rm)
```

```{r}
boston.age <- lm(crim ~ age, data = Boston)
summary(boston.age)
#plot(boston.age)
```

```{r}
boston.dis <- lm(crim ~ dis, data = Boston)
summary(boston.dis)
#plot(boston.dis)
```

```{r}
boston.rad <- lm(crim ~ rad, data = Boston)
summary(boston.rad)
#plot(boston.rad)
```

```{r}
boston.tax <- lm(crim ~ tax, data = Boston)
summary(boston.tax)
#plot(boston.tax)
```

```{r}
boston.ptratio <- lm(crim ~ ptratio, data = Boston)
summary(boston.ptratio)
#plot(boston.ptratio)
```

```{r}
boston.lstat <- lm(crim ~ lstat, data = Boston)
summary(boston.lstat)
#plot(boston.lstat)
```


```{r}
boston.medv <- lm(crim ~ medv, data = Boston)
summary(boston.medv)
#plot(boston.medv) 
```
```{r}
#Create some plots to back up your assertions.
plot(Boston$zn, Boston$crim)
abline(boston.zn, col = "red")

plot(Boston$dis, Boston$crim)
abline(boston.dis, col = "red")

plot(Boston$age, Boston$crim)
abline(boston.age, col = "red")

plot(Boston$nox, Boston$crim)
abline(boston.nox, col = "red")

plot(Boston$lstat, Boston$crim)
abline(boston.lstat, col = "red")

#no linear relationship between crim and chas
plot(Boston$chas, Boston$crim)
abline(boston.chas, col = "red")


```

Chas, which indicates whether the census tract bounds the Charles River, has a p-value of 0.2094, which is greater than 0.05, so we fail to reject the null hypothesis. When fitting a simple linear regression model for the rest of the predictor variables, all have a p=value less than 0.05, which means that there is a statistically significant association between the predictor and the response. As shown in the plots above, there is a negative linear relationship between the predictor and the response (crim) for zn and dis, a positive relationship for age, nox,and lstat, but no linear relationship between crim and chas.

##### b. Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?

```{r}
boston.multiple <- lm(crim ~ ., data = Boston)
summary(boston.multiple)
```
For the multiple regression model, the predictors zn, dis, rad, and medv have p-values less than 0.05, which means that we can reject the null hypothesis H0 : βj = 0 for these predictors. The other predictors all have p-values greater than 0.05, so we cannot reject the null hypothesis H0 : βj = 0 for these predictors.

##### c. How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

There are fewer predictors showing statistical significance with p-values less than 0.05 in the multiple regression model (part b) than in the simple linear regression models (part a). The plot below shows the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. 

```{r}
#Univariate coefficients

univ_coeff = c(coefficients(boston.zn)[2],
               coefficients(boston.indus)[2],
               coefficients(boston.chas)[2],
               coefficients(boston.nox)[2],
               coefficients(boston.rm)[2],
               coefficients(boston.age)[2],
               coefficients(boston.dis)[2],
               coefficients(boston.rad)[2],
               coefficients(boston.tax)[2],
               coefficients(boston.ptratio)[2],
               coefficients(boston.lstat)[2],
               coefficients(boston.medv)[2])
multi_coeff = coefficients(boston.multiple)[2:13]
plot(univ_coeff, multi_coeff, xlab = "Univariate Coefficients", ylab = "Multiple Coefficients")

print(cbind(univ_coeff,multi_coeff))
```
Most of the points are clustered together around the origin, which means that the coefficients are similar in both the simple linear regression models and the multiple regression model. However, the multiple regression coefficients are generally smaller than the univariate regression coefficients. There is also one predictor with drastic differences in coefficients, nox, which has a coefficient of 31.25 in the simple linear regression model, but a coefficient of -9.96 in the multiple regression model. It is possible that the nox predictor is correlated with other predictors in the multiple regression model, which could explain the big difference in coefficients.

##### d. Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form

$$
Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon.
$$

```{r}
lm.zn <- lm(crim ~ poly(zn, 3), data = Boston)
summary(lm.zn)
#poly in this case creates a set of orthogonal polynomials up to degree 3 for the given predictor variable X
```

```{r}
lm.indus <- lm(crim ~ poly(indus, 3), data = Boston)
summary(lm.indus)
```

```{r}
lm.nox <- lm(crim ~ poly(nox, 3), data = Boston)
summary(lm.nox)
```

```{r}
lm.rm <- lm(crim ~ poly(rm, 3), data = Boston)
summary(lm.rm)
```

```{r}
lm.age <- lm(crim ~ poly(age, 3), data = Boston)
summary(lm.age)
```

```{r}
lm.dis <- lm(crim ~ poly(dis, 3), data = Boston)
summary(lm.dis)
```

```{r}
lm.rad <- lm(crim ~ poly(rad, 3), data = Boston)
summary(lm.rad)
```

```{r}
lm.tax <- lm(crim ~ poly(tax, 3), data = Boston)
summary(lm.tax)
```

```{r}
lm.ptratio <- lm(crim ~ poly(ptratio, 3), data = Boston)
summary(lm.ptratio)
```

```{r}
lm.lstat <- lm(crim ~ poly(lstat, 3), data = Boston)
summary(lm.lstat)
```

```{r}
lm.medv <- lm(crim ~ poly(medv, 3), data = Boston)
summary(lm.medv)
```
Yes there is evidence for many of the variables exhibiting non-linear association between the predictors and the response. The model with the polynomial term X^3 makes the following variables significant: indus, nox, age, dis, ptratio, medv as the results show that Pr(>|t|) < 0.05 for the row with the cubic term (poly(predictor, 3)3). The other predictor variables that do not show statistical signficiance with a cubic term still show evidence of non-linear association between the predictors and the response, but with the squared term. The predictors that show Pr(>|t|) < 0.05 for the row with the squared term (poly(predictor, 3)2) are zn, rm, rad, tax, and lstat.

## Bonus question (20pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$

