---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "Kathy Hoang and 506333118"
date: 02/13/2024
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---
```{r}
#Clear Environment
rm(list=ls())
```

```{r}
#LLoad Libraries
library(ISLR2)
library(boot) #library for bootstrapping
library(MASS)
library(leaps)
library(glmnet)
library(pls) #PCR

#libraries from lecture notes
library(GGally)
library(ISLR2)
library(tidyverse)
library(tidyverse)
library(tidymodels)
```

## ISL Exercise 5.4.2 (10pts)
We will now derive the probability that a given observation is part
of a bootstrap sample. Suppose that we obtain a bootstrap sample
from a set of n observations.

### 5.4.2.a
What is the probability that the first bootstrap observation is
not the jth observation from the original sample? Justify your
answer.

**Answer**
$$
1 - \frac{1}{n}
$$

The probability that the first bootstrap observation is not the jth observation from the original sample is $1 - \frac{1}{n}$.

The jth observation can be represented as 1/n because only 1 out of total n observations is the jth observation. Thus, it we take a total of n out of n (n/n) observations, which is equivalent to 1, and subtract the probability of the jth observation, we get 1 - (1/n). In other words, this is the probability that the first bootstrap observation is any of the n observations from the original sample except for the jth observation.

### 5.4.2.b
What is the probability that the second bootstrap observation
is not the jth observation from the original sample?

**Answer**
$$
1 - \frac{1}{n}
$$

The probability that the second bootstrap observation is not the jth observation from the original sample is $1 - \frac{1}{n}$. This is the same answer as part a because it does not matter if it is the first or second bootstrap observation, the second bootstrap observation is independent of the first bootstrap observation. Thus, the probability of the jth observation is still 1/n.

### 5.4.2.c
Argue that the probability that the jth observation is not in the 
bootstrap sample is $(1 - \frac{1}{n})^n $.

**Answer**
If we do not want the jth observation to be in the bootstrap sample, then each individual observation would have a probability of $1 - \frac{1}{n}$ for all of the n observations. Since the bootstrap sample is drawn with replacement and each bootstrap observation is independent of each other, we would just multiply this probability by itself n times. So for example, the first independent observation would have a probability of $1 - \frac{1}{n}$, the second independent observation would have a probability of $1 - \frac{1}{n}$, etc up to the nth independent observation. Therefore, the probability of the jth observation not being in the bootstrap sample is $(1 - \frac{1}{n})^n $.

### 5.4.2.d
When n = 5, what is the probability that the jth observation is
in the bootstrap sample?

**Answer**
Using the formula given in part c, we can subtract the probability that the jth observation $\textbf{is not in}$ the bootstrap sample from 1 to get the probability that the jth observation $\textbf{is in}$ the bootstrap sample: 
$1 - (1 - \frac{1}{n})^n$. Plugging in n = 5, we can calculate :

$$
\begin{align*}
&= 1 - (1 - \frac{1}{n})^n \\
&= 1 - (1 - \frac{1}{5})^5 \\
&=  1 - (\frac{4}{5})^5 \\
&= 0.67232 \\
\end{align*}
$$
The probability of the jth observation being in the bootstrap sample when n = 5 is 67.23%.

### 5.4.2.e
When n = 100, what is the probability that the jth observation
is in the bootstrap sample?
**Answer**
Similar to part d, we can plug in n=100 to get:
$$
\begin{align*}
&= 1 - (1 - \frac{1}{n})^{n} \\
&= 1 - (1 - \frac{1}{100})^{100} \\
&=  1 - (\frac{99}{100})^{100} \\
&= 0.6339677 \\
\end{align*}
$$
The probability of the jth observation being in the bootstrap sample when n = 100 is about 63.4%, which is slightly less than the probability of jth observation being in the bootstrap sample when n = 5 (from part d).

### 5.4.2.f
When n = 10000, what is the probability that the jth observation
is in the bootstrap sample?

**Answer**
We can plug in n= 10000 to get:
$$
\begin{align*}
&= 1 - (1 - \frac{1}{n})^{n} \\
&= 1 - (1 - \frac{1}{10000})^{10000} \\
&=  1 - (\frac{9999}{10000})^{10000} \\
&= 0.632139 \\
\end{align*}
$$
The probability of the jth observation being in the bootstrap sample when n = 10000 is approximately 63.21%.

### 5.4.2.g
Create a plot that displays, for each integer value of n from 1
to 100000, the probability that the jth observation is in the
bootstrap sample. Comment on what you observe.

**Answer**
```{r}
prob_jth_in_fn <- function(n) {
  return(1 - (1 - 1/n)^n)
}
x <- 1:100000
y <- sapply(x, prob_jth_in_fn)

plot(x, y,type = "o", xlab = "n", ylab = "Probability", main = "Probability of jth Observation in Bootstrap Sample")
```
The plot shows that as n increases, the probability that the jth observation is in the bootstrap sample converges to approximately 63.21%. This is consistent with the results shown in the previous problems because when n is large, we observed the probability to continually approach closer to 63.21%. 

Note: 1 - (99999/100000)**100000 = 0.6321224

### 5.4.2.h
We will now investigate numerically the probability that a bootstrap
sample of size n = 100 contains the jth observation. Here
j = 4. We repeatedly create bootstrap samples, and each time
we record whether or not the fourth observation is contained in
the bootstrap sample.

Comment on the results obtained.

```{r}
store <- rep (NA , 10000)
  for (i in 1:10000) {
store [i] <- sum (sample (1:100, rep = TRUE) == 4) > 0
}
mean(store)
```
**Answer**

The probability that a bootstrap sample of size n = 100 contains the jth observation (j = 4) is 0.6345. In part e, we calculated the probability of the jth observation being in the bootstrap sample when n = 100 to be 0.6339677. These numbers are extremely close to each other as they only differ by 0.0005323 (0.6345-0.6339677 = 0.0005323). This suggests that the probability estimated with the bootstrap sampling method is consistent with the theoretical probability and that this resampling method is a good approximation of the true probability.

## ISL Exercise 5.4.9 (20pts)
We will now consider the Boston housing data set, from the ISLR2
library.

### 5.4.9.a
Based on this data set, provide an estimate for the population
mean of medv. Call this estimate $\hat{\mu}$.

**Answer**
```{r}
df = Boston
mu_hat = mean(df$medv)
mu_hat
```
The estimation for the population mean of medv, $\hat{\mu}$, is 22.53281.

### 5.4.9.b
Provide an estimate of the standard error of $\hat{\mu}$. Interpret this
result.

Hint: We can compute the standard error of the sample mean by
dividing the sample standard deviation by the square root of the
number of observations.

**Answer**
```{r}
se_mu_hat = sd(df$medv)/sqrt(length(df$medv))
se_mu_hat
```
The standard error of the sample mean is 0.4088611. This means that the sample mean is expected to vary from the true population mean by approximately 0.4088611 units on average due to random sampling variability.

### 5.4.9.c
Now estimate the standard error of $\hat{\mu}$ using the bootstrap. How
does this compare to your answer from (b)?

**Answer**
```{r}
set.seed(212)
boot_fn_mean <- function(data, index) return(mean(data[index])) 
# function to compute the mean of the bootstrap sample
#takes in 2 arguments, index of the bootstrap sample and data

bs <- boot(df$medv, boot_fn_mean, 1000) # 1000 bootstrap samples
bs
```
The standard error using the bootstrap method is about 0.39. This is close to the standard error computed from part (b) 0.4088611, as it is only about 0.015 different. This is expected as the standard error of the sample mean via the traditional method (computed using sample statistics) is expected to be close to the standard error of the bootstrap method (which simulates the sampling process through resampling).

### 5.4.9.d
Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95% confidence interval using the
formula $[\hat{\mu} - 2SE(\hat{\mu}),\hat{\mu} + 2SE(\hat{\mu})]$

**Answer**
```{r}
se_mu_hat <- sd(bs$t) #SE from part c
se_mu_hat
ci = c(mu_hat - 2*se_mu_hat, mu_hat + 2*se_mu_hat)
ci

t.test(Boston$medv)

```
The 95% Confidence Interval for the mean of medv is [21.74527, 23.32034]. This is very close to the results obtained using t.test(Boston$medv) which is [21.72953, 23.33608]. The difference is only about 0.01574 away as 21.74527-21.72953 = 0.01574 and 23.32034-23.33608 = 0.01574.

### 5.4.9.e
Based on this data set, provide an estimate, $\hat{\mu}_{med}$, for the median
value of medv in the population.

**Answer**
```{r}
mu_hat_med = median(df$medv)
mu_hat_med
```
The estimation for the median value of medv in the population, $\hat{\mu}_{med}$, is 21.2.

### 5.4.9.f
We now would like to estimate the standard error of  $\hat{\mu}_{med}$. Unfortunately,
there is no simple formula for computing the standard
error of the median. Instead, estimate the standard error of the 
median using the bootstrap. Comment on your findings.

**Answer**
```{r}
set.seed(212)
boot_med_fn <- function(data, index) return(median(data[index])) #use median
bs_med <- boot(df$medv, boot_med_fn, 1000) # 1000 bootstrap samples
bs_med
se_mu_hat_med <- sd(bs_med$t) #SE from part f
se_mu_hat_med
```

The standard error using the bootstrap method is about 0.3707078. This is relatively small as the median value is 21.2 This standard error of the median is also slightly smaller compared to the standard error of the sample mean using the boostrap method (from part c) which is 0.3937688. This could be because the median is more robust to outliers and less affected by the skewedness of the data compared to the mean.

### 5.4.9.g
Based on this data set, provide an estimate for the tenth percentile
of medv in Boston census tracts. Call this quantity $\hat{\mu}_{0.1}$.
(You can use the quantile() function.)

**Answer**
```{r}
mu_hat_tenth_perc = quantile(df$medv, 0.1)
mu_hat_tenth_perc
```

### 5.4.9.h
Use the bootstrap to estimate the standard error of $\hat{\mu}_{0.1}$. Comment
on your findings.

**Answer**
```{r}
set.seed(212)
boot_fn_tenth <- function(data, index) return(quantile(data[index], 0.1)) #quantile
#use 10th percentile
bs_tenth_perc <- boot(df$medv, boot_fn_tenth, 1000) # 1000 bootstrap samples
bs_tenth_perc
```
The standard error of $\hat{\mu}_{0.1}$ using the bootstrap method is about 0.4921582. This standard error of the 10th percentile is larger compared to the standard error of the median using the bootstrap method (from part f) which is 0.3707078. This comes to be a difference of 0.12 between the standard errors. This could be because the 10th percentile is more sensitive to the lower end of the data and is more affected by the skewedness of the data compared to the median. Regardless, the standard error of the 10th percentile is still relatively small compared to the 10th percentile value of 12.75.

## Least squares is MLE (10pts)
Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

$$
\begin{align*}
&\text{Given a linear model with Gaussian errors } \\
y_i &~ N(\beta x_i, \sigma^2) \\
&\text{The probability density function of Gaussian } \\
P(y_i) &= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \hat{y_i})^2}{2\sigma^2}} \\

&\text{the likelihood function is:} \\
L(\beta_0 \mid x_i, y_i) &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \hat{y_i})^2}{2\sigma^2}} \\

&\text{Taking the log of the likelihood function:} \\
log(L) &= \sum_{i=1}^n log(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \hat{y_i})^2}{2\sigma^2}}) \\
&= \sum_{i=1}^n log(\frac{1}{\sqrt{2\pi\sigma^2}}) + \sum_{i=1}^n log(e^{-\frac{(y_i - \hat{y_i})^2}{2\sigma^2}}) \\
&= \sum_{i=1}^n log(\frac{1}{\sqrt{2\pi\sigma^2}}) - \sum_{i=1}^n \frac{(y_i - \hat{y_i})^2}{2\sigma^2} \\
&= -\frac{n}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2 \\
&= -\frac{n}{2}log(2\pi) - \frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2 \\
&\text{Taking the derivative of the log likelihood function with respect to variance } \\ &\sigma^2 \text{ and setting it equal to 0:} \\
&\frac{\partial}{\partial \sigma^2} log(L) = 0 \\
\frac{\partial}{\partial \sigma^2} &(-\frac{n}{2}log(2\pi) - \frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2) = 0 \\
\frac{\partial}{\partial \sigma^2} &(-\frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2) = 0 \\
-\frac{n}{2\sigma^2} &+ \frac{1}{2\sigma^4}\sum_{i=1}^n (y_i - \hat{y_i})^2 = 0 \\
\frac{n}{2\sigma^2} &= \frac{1}{2\sigma^4}\sum_{i=1}^n (y_i - \hat{y_i})^2 \\
\frac{n}{\sigma^2} &= \frac{1}{\sigma^4}\sum_{i=1}^n (y_i - \hat{y_i})^2 \\
n\sigma^2 &= \sum_{i=1}^n (y_i - \hat{y_i})^2 \\
\sigma^2 &= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y_i})^2 \\
0 &= \sum_{i=1}^n (y_i - \hat{y_i})^2 \\
&\text{This is the same as the least squares estimate of the variance} \\
\end{align*}
$$
$$
\begin{align*}
\\
&\text{Now plugging this into AIC and } C_p\\
\\
&\text{Recall that: } \\
AIC &= -2log(L) + 2d \\
&\text{where L is the maximized value of the likelihood function for the estimated model.} \\
C_p &= \frac{1}{n}(RSS + 2d\hat{\sigma}^2) \\
&\text{where d is the total number of parameters used and } \hat{\sigma}^2 \text{ is an estimate of the error variance} \\
\\
&\text{Starting with AIC:} \\
AIC &= -2log(L) + 2d \\
&\text{Sub L with the maximized log likelihood function we obtained above:} \\
AIC &= -2(-\frac{n}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2) + 2d \\
&= nlog(2\pi\sigma^2) + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \hat{y_i})^2 + 2d \\
&\text{We know that RSS = } \sum_{i=1}^n (y_i - \hat{y_i})^2 \\
AIC &= nlog(2\pi\sigma^2) + \frac{RSS}{\sigma^2} + 2d \\
\\
&\text{Recall that} \\
C_p &= \frac{1}{n}(RSS + 2d\hat{\sigma}^2) \\
nC_p &= RSS + 2d\hat{\sigma}^2 \\

&= nlog(2\pi\sigma^2) + \frac{n}{\sigma^2}C_p \\
&\text{Since they only differ by a constant and are a function of constants n and } \hat\sigma^2\\
&\text{the selection of the model is not affected by using AIC or }C_p \\
&\text{Thus , AIC and } C_p \text{ are equivalent.} \\
\end{align*}
$$

## ISL Exercise 6.6.1 (10pts)
We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1
models, containing 0,1,2,...,p predictors. Explain your answers:

### 6.6.1.a 
Which of the three models with k predictors has the smallest training RSS?

**Answer**

Out of the three models, the best subset selection model with k predictors has the smallest training RSS. Subset selection evaluates all the $p \choose k$ possible combinations of predictors for each k and selects the model with the smallest training RSS. However, for forward and backward stepwise selection, the order which the predictors are added or removed is based on the smallest training RSS at each step. While these models aim to converge to the best model with the lowest RSS, the path they take is not exhaustive of all possible combinations of predictors so it is not guaranteed that the model with k predictors has the smallest training RSS.

Source: ISL pg. 227, 232: "As we discussed in Section 6.1.1, the model containing ALL of the predictors will always have the smallest RSS and the largest R2, since these quantities are related to the training error"

### 6.6.1.b
Which of the three models with k predictors has the smallest
test RSS?

**Answer**

As we learned in Chapter 2, training error is a poor estimate of the test error. RSS and R^2 are not suitable for selecting the best model with respect to test error as they are metrics used to evaluate the performance of models on training data, not unseen test data. Therefore, we will not be able to determine which of the three models with k predictors has the smallest test RSS without evaluating the test RSS for each model. However, since best subset selection evaluates all the possible models, there is a higher chance that it may have the smallest test RSS for the model, though it is not guaranteed. 

Source: ISL pg. 232

### 6.6.1.c
True or False:

#### i. 

The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.

**Answer: True**
This is true because forward stepwise selection adds a predictor every iteration, so the (k+1)-variable model will contain the k-variable model and an additional predictor. Thus, the k-variable model is a subset of the (k+1)-variable model.

#### ii: 

The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection.

**Answer: True.**
This is true because backward stepwise selection starts with all the predictors and removes a predictor every iteration, so the k-variable model will contain all predictors from the (k+1)-variable model minus one predictor. Thus, the k-variable model is still a subset of the (k+1)-variable model.

#### iii: 

The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-
variable model identified by forward stepwise selection.

**Answer: False.**
This is false because forward stepwise selection and backward stepwise selection may not necessarily result in the same model. It is possible for the k-variable model identified by backward stepwise to have a different set of predictors than the (k+1)-variable model identified by forward stepwise selection as there are different combinations/paths they can take. For example, even after removing a few predictors, the resulting k-variable model identified by backward stepwise selection could still have more predictors that the (k+1)-variable model identified by forward stepwise selection did not add.Thus, the predictors in the k-variable model identified by backward stepwise selection is not guranteed to be a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.

#### iv: 

The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.

**Answer: False.**
As mentioned in part iii, forward stepwise selection and backward stepwise selection may not necessarily result in the same model. For example, the k-variable model identified by forward stepwise selection may add on more predictors that the (k+1)-variable model identified by backward stepwise selection already removed. Thus, the predictors in the k-variable model identified by forward stepwise selection is not guranteed to be a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.

#### v:
The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.

**Answer:  False**

This is false because best subset selection evaluates all the possible models and selects the best model for each k. The (k+1) variable model may not necessarily contain all the predictors selected for the k-variable model. Therefore, the k-variable model identified by best subset selection is not guaranteed to be a subset of the predictors in the (k+1)-variable model identified by best subset selection.

## ISL Exercise 6.6.3 (10pts)
Suppose we estimate the regression coefficients in a linear regression
model by minimizing for a particular value of s. For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

$$
\begin{align*}
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \text{ subject to } \sum_{j=1}^p \mid \beta_j \mid \le s
\end{align*}
$$
Options: 

#### i. 
Increase initially, and then eventually start decreasing in an inverted U shape.

#### ii. 
Decrease initially, and then eventually start increasing in a U shape.

#### iii. 
Steadily increase.

#### iv. 
Steadily decrease.

#### v. 
Remain constant.

Note: Refer back to sketch in hw1


### 6.6.3.a 
As we increase s from 0, the training RSS will:

**Answer: iv. Steadily decrease** As s increases from 0, the constraint becomes less restrictive, and the sum of absolute values of the coefficients can be higher. As a result, the model will be able to fit the training data better and be more flexible, so the training RSS will steadily decrease. 

### 6.6.3.b
Repeat (a) for test RSS.

**Answer: ii. Decrease initially, and then eventually start increasing in a U shape.** When s=0, all B_j = 0, so the model is too simple and the test RSS is high. As s increases from 0, B_j will take on non-zero values and the model will be able to fit on test data better and be more flexible, so the test RSS will decrease at first. However, as s continues to increase, the model will become too flexible and overfit the training data, so the test RSS will eventually start increasing in a U shape. This is because the model will start to fit the noise in the training data and not generalize well to the test data. 

### 6.6.3.c
Repeat (a) for variance.

**Answer: iii. Steadily increase.** As s increases from 0, the model will be able to fit the training data better and be more flexible, so the variance will steadily increase. Recall from hw1 that as flexibility increases, variance will also increase because the model is more sensitive to fluctuations in the training set. In general, more flexible statistical methods have higher variance. 

### 6.6.3.d
Repeat (a) for (squared) bias.

**Answer: iv. Steadily decrease.** When s=0, there is no penalty term on the B_j coefficients, so the model is too simple and the bias is high. As s increases from 0, the model will be able to fit the training data better and be more flexible, so the bias will steadily decrease. 

### 6.6.3.e
Repeat (a) for the irreducible error.
**Answer: v. Remain constant** The irreducible error is independent of the model and will remain constant as s increases from 0.

## ISL Exercise 6.6.4 (10pts)
Suppose we estimate the regression coefficients in a linear regression
model by minimizing
$$
\begin{align*}
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2
\end{align*}
$$

for a particular value of $\lambda$ . For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

Options: 

#### i. 
Increase initially, and then eventually start decreasing in an inverted U shape.

#### ii. 
Decrease initially, and then eventually start increasing in a U shape.

#### iii. 
Steadily increase.

#### iv. 
Steadily decrease.

#### v. 
Remain constant.

To minimize, we take the derivative of the equation with respect to $\lambda$ and set it equal to 0. 
$$
\begin{align*}
\frac{\partial}{\partial \lambda} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2 &= 0 \\
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \sum_{j=1}^p \beta_j^2 &= 0 \\

\end{align*}
$$
Notes (reminders to help me understand the problem):
- When $\lambda$ = 0,the Ridge Regression Line will be the same as the Least Squares Line because they are both minimizing the same thing - the sum of the squared residuals.
- The large $\lambda$ gets, the smaller the slope (penalty) gets and the slope asymptotically approaches closer to 0.
- Ridge Regression adds a little bias, but it greatly reduces the variance by shrinking parameters and making predictions less sensitive to them (reduces the weight each independent var has on the output)
- the point of Ridge Regression is because small sample sizes can lead to poor least squares estimates that result in poor machine learning predictions (esp when num predictors > num obs)

### 6.6.4.a 
As we increase $\lambda$ from 0, the training RSS will:

**Answer: iii. steadily increase**When $\lambda$= 0, there is no penalty term (Since $\lambda$ determines how severe the penalty is and the product of 0 * penalty is just 0), and the model is too flexible, so the training RSS is low. As $\lambda$ increases from 0, the penalty term becomes more restrictive and the coefficients will shrink. As a result, the model will become less flexible and the training RSS will steadily increase. 

### 6.6.4.b

Repeat (a) for test RSS.

**Answer: #### ii. Decrease initially, and then eventually start increasing in a U shape.** When $\lambda$ = 0, the test RSS is high because the model is too flexible and likely to overfit. The test RSS will decrease initially as the model becomes less flexible and the coefficients shrink. However, as $\lambda$ continues to increase, the model will become too inflexible and the test RSS will eventually start increasing in a U shape. This is because the model will be too simple and underfit, so the test RSS will increase again.

### 6.6.4.c
Repeat (a) for variance.

**Answer: iv. steadily decrease** When $\lambda$ = 0, there is no penalty term and the model is too flexible, so the variance is high. As $\lambda$ increases from 0, the penalty term becomes more restrictive and the B_j coefficients will decrease. As a result, the model will become less flexible and the variance will steadily decrease. This is one of the benefits of Ridge Regression - it reduces the variance of the model.

### 6.6.4.d

Repeat (a) for (squared) bias.
**Answer: iii. steadily increase** When $\lambda$ = 0, there is no penalty term and the model is too flexible, so the bias is low. As $\lambda$ increases from 0, the penalty term becomes more restrictive and the B_j coefficients will shrink. As a result, the model will become more simple and the bias will steadily increase. (Notice bias-variance tradeoff with part c)

### 6.6.4.e

Repeat (a) for the irreducible error.
**Answer: v. Remain constant** The irreducible error is independent of the model and will remain constant as $\lambda$ increases from 0.

## ISL Exercise 6.6.5 (10pts)

It is well-known that ridge regression tends to give similar coefficient
values to correlated variables, whereas the lasso may give quite different
coefficient values to correlated variables. We will now explore
this property in a very simple setting.

Suppose that n = 2, p = 2, x11 = x12, x21 = x22. Furthermore,
suppose that y1+y2 = 0 and x11+x21 = 0 and x12+x22 = 0, so that
the estimate for the intercept in a least squares, ridge regression, or
lasso model is zero: $\hat\beta_0 = 0$.

### 6.6.5.a 
Write out the ridge regression optimization problem in this setting.

**Answer**
$$
\begin{align*}
& \text{Recall that Ridge Regression optimization appears like:} \\
\text{minimize } & \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2 \\
& \text{Plug in } n  = 2, p = 2, \hat\beta_0 = 0 \\
\text{minimize } & \sum_{i=1}^2 (y_i - 0 - \sum_{j=1}^2 \beta_j x_{ij})^2 + \lambda \sum_{j=1}^2 \beta_j^2 \\
& i=1: (y_1 - 0 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 \\
& i=2: (y_2 - 0 - \hat\beta_1x_{21} - \hat\beta_2x_{22})  \\
& \lambda (\hat\beta_1^2) + \lambda (\hat\beta_2^2) \\
& \text{Combining and simplifying, we get:} \\
\text{minimize } & 
(y_1 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 + (y_2 - \hat\beta_1x_{21} - \hat\beta_2x_{22})^2 + \lambda (\hat\beta_1^2 + \hat\beta_2^2) \\

\end{align*}
$$

### 6.6.5.b
Argue that in this setting, the ridge coefficient estimates satisfy
 $\hat\beta_1 = \hat\beta_2$.
 
$$
\begin{align*}
&\text{Since we were given } x_{11} = x_{12} \text{ and }  x_{21} = x_{22}, \text{} \text{we will refer to them as }
x_1 \text{ and } x_2 \text{ respectively: }\\
& x_{11} = x_{12} = x_1 \\
& x_{21} = x_{22} = x_2 \\
&\text{We can rewrite the ridge regression optimization problem as:} \\
&\text{minimize } (y_1 - \hat\beta_1x_1 - \hat\beta_2x_1)^2 + (y_2 - \hat\beta_1x_2 - \hat\beta_2x_2)^2 + \lambda (\hat\beta_1^2 + \hat\beta_2^2) \\
&\text{Taking the partial derivatives with respect to } \hat\beta_1 \text{and setting it equal to 0}: \\
&\frac{\partial}{\partial \hat\beta_1} = -2x_1(y_1 - \hat\beta_1x_1 - \hat\beta_2x_1) - 2x_2(y_2 - \hat\beta_1x_2 - \hat\beta_2x_2) + 2\lambda\hat\beta_1 = 0\\
&2x_1y_1 - 2\hat\beta_1x_1^2 - 2\hat\beta_2x_1^2 - 2x_2y_2 -2x_2y_2 + 2\hat\beta_1x_2x_2 + 2\hat\beta_2x_2x_2 + 2\lambda\hat\beta_1 = 0 \\
&\beta_1(x_1^2 + x_2^2 ) + \hat\beta_2(x_1^2 + x_2^2) + \lambda\hat\beta_1 = x_1y_1 + x_2y_2 \\
&\beta_1(x_1^2 + x_2^2 + \lambda) + \hat\beta_2(x_1^2 + x_2^2)  = x_1y_1 + x_2y_2 \\
&\text{solving for } \hat\beta_1 \\
&\hat\beta_1(x_1^2 + x_2^2 + \lambda) = x_1y_1 + x_2y_2 - \hat\beta_2(x_1^2 + x_2^2) \\

&\hat\beta_1 = \frac{x_1y_1 + x_2y_2 - \hat\beta_2(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} \\

&\text{Now taking the partial derivatives with respect to } \hat\beta_2 \text{and setting it equal to 0}: \\
&\frac{\partial}{\partial \hat\beta_2} = -2x_1(y_1 - \hat\beta_1x_1 - \hat\beta_2x_1) - 2x_2(y_2 - \hat\beta_1x_2 - \hat\beta_2x_2) + 2\lambda\hat\beta_2 = 0\\
&2x_1y_1 - 2\hat\beta_1x_1^2 - 2\hat\beta_2x_1^2 - 2x_2y_2 -2x_2y_2 + 2\hat\beta_1x_2x_2 + 2\hat\beta_2x_2x_2 + 2\lambda\hat\beta_2 = 0 \\
&\beta_1(x_1^2 + x_2^2 ) + \hat\beta_2(x_1^2 + x_2^2) + \lambda\hat\beta_2 = x_1y_1 + x_2y_2 \\
&\beta_1(x_1^2 + x_2^2) + \hat\beta_2(x_1^2 + x_2^2+ \lambda))  = x_1y_1 + x_2y_2 \\
&\text{solving for } \hat\beta_2 \\
&\hat\beta_2(x_1^2 + x_2^2 + \lambda) = x_1y_1 + x_2y_2 - \hat\beta_1(x_1^2 + x_2^2) \\
&\hat\beta_2 = \frac{x_1y_1 + x_2y_2 - \hat\beta_1(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} 
\\
\end{align*}
$$

$$
\begin{align*}
&\hat\beta_1 = \frac{x_1y_1 + x_2y_2 - \hat\beta_2(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} 
= 0 \\
&\hat\beta_2 = \frac{x_1y_1 + x_2y_2 - \hat\beta_1(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} 
= 0 \\
&\text{Setting them equal to each other (since they both =0):}\\
&\frac{x_1y_1 + x_2y_2 - \hat\beta_2(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} = 
\frac{x_1y_1 + x_2y_2 - \hat\beta_1(x_1^2 + x_2^2)}{x_1^2 + x_2^2 + \lambda} \\
&x_1y_1 + x_2y_2 - \hat\beta_2(x_1^2 + x_2^2) = x_1y_1 + x_2y_2 - \hat\beta_1(x_1^2 + x_2^2) \\
&\hat\beta_1(x_1^2 + x_2^2) = \hat\beta_2(x_1^2 + x_2^2) \\
&\hat\beta_1 = \hat\beta_2 \\
&\text{Thus, we see that } \hat\beta_1 \text{ and } \hat\beta_2 \text{ are equal.} \\
\end{align*}
$$

### 6.6.5.c

Write out the lasso optimization problem in this setting.
$$
\begin{align*}
& \text{Recall that Lasso optimization appears like:} \\
\text{minimize } & \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \mid \beta_j \mid \\
& \text{Plug in } n  = 2, p = 2, \hat\beta_0 = 0 \\
\text{minimize } & \sum_{i=1}^2 (y_i - 0 - \sum_{j=1}^2 \beta_j x_{ij})^2 + \lambda \sum_{j=1}^2 \mid \beta_j \mid \\
& i=1: (y_1 - 0 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 \\
& i=2: (y_2 - 0 - \hat\beta_1x_{21} - \hat\beta_2x_{22})  \\
& \lambda (\mid \hat\beta_1 \mid + \mid \hat\beta_2 \mid) \\
& \text{Combining and simplifying we get:} \\
\text{minimize } & 
(y_1 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 + (y_2 - \hat\beta_1x_{21} - \hat\beta_2x_{22})^2 + \lambda (\mid \hat\beta_1 \mid + \mid \hat\beta_2 \mid) \\

\end{align*}
$$

### 6.6.5.d
Argue that in this setting, the lasso coefficients $\hat\beta_1$ and $\hat\beta_2$ 
are not unique - in other words, there are many possible solutions
to the optimization problem in (c). Describe these solutions.

**Answer**
Solving for a system of equations
$$
\begin{align*}
&\text{Given} \\
x_{11} &= x_{12} \\
x_{21} &= x_{22} \\
x_{11} &+ x_{21} = 0 \\
x_{12} &+ x_{22} = 0 \\
y_1 &+ y_2 = 0 \\

\text{minimize } 2(y_1 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 &+ 2(y_2 - \hat\beta_1x_{21} - \hat\beta_2x_{22})^2 + \lambda (\mid \hat\beta_1 \mid + \mid \hat\beta_2 \mid) \\
= \text{minimize } 2(y_1 - (\hat\beta_1 + \hat\beta_2)x_{11})^2) \\
\text{The solution to the optimization is }
\hat\beta_1 + \hat\beta_2 = \frac{y_1}{x_{11}}
\end{align*}

$$

## ISL Exercise 6.6.11 (30pts)
We will now try to predict per capita crime rate in the Boston data
set.

### 6.6.11.a
Try out some of the regression methods explored in this chapter,
such as best subset selection, the lasso, ridge regression, and
PCR. Present and discuss results for the approaches that you
consider.

```{r}
#Make a copy of Boston dataframe
df = Boston
head(df,5)

#Check for missing values
sum(is.na(df))
```
```{r}
#Split data into training and test set
set.seed(17)
train = df %>% sample_frac(0.5) 
test = df %>% setdiff(train)
#Note: tried to split data diff (ex. 0.75) but this seemed to work better
#when splitting 50/50, LS is higher error which shows the other models improved

#Create x and y variables for training and test set
train_x = model.matrix(crim ~ ., train)[,-1]
test_x = model.matrix(crim ~ ., test)[,-1]
#-1 to remove intercept
train_y = train$crim
test_y = test$crim
```

#### Best Subset Selection Visualize - pulled from lecture notes
```{r}
# Fit best subset regression
bst_mod <- regsubsets(
  crim ~ ., 
  data = df, 
  method = "exhaustive",
  nvmax = 11
  ) %>% summary()
bst_mod

# Display selection criteria
bst_result <- tibble(
  K = 1:11, 
  R2 = bst_mod$rsq,
  adjR2 = bst_mod$adjr2,
  BIC = bst_mod$bic,
  CP = bst_mod$cp
) %>% print(width = Inf)

# Visualize
cols <- names(bst_result)
for (j in 2:5) {
  (bst_result %>%
  select(K, !!sym(cols[j])) %>%
  ggplot() + 
    geom_line(mapping = aes(x = K, y = !!sym(cols[j]))) + 
    labs(x = 'Model Size', y = cols[j])) %>%
  print()
}
#Source:ISL6 Lecture Notes
```

#### Best Subset Selection
```{r}
best_sub_mod = regsubsets(train_y ~ ., data = train, nvmax = 11)
reg_summary = summary(best_sub_mod)
names(reg_summary)

#Best Subset Model with Lowest BIC
best_subset_bic = which.min(summary(best_sub_mod)$bic)
best_subset_bic

#Best Subset Model with Lowest ADJR2
best_subset_adjr2 = which.min(summary(best_sub_mod)$adjr2)
best_subset_adjr2
```

#### Ridge Regression
```{r}
#Set up Ridge Model
lambda_grid <- 10^seq(10, -2, length.out = 100)
#Cross-validation to find the best lambda
ridge_mod = cv.glmnet(train_x, train_y, alpha = 0, lambda = lambda_grid)
#alpha = 0 is ridge regression, default is 1 lasso
plot(ridge_mod)
lambda_best_ridge = ridge_mod$lambda.min
lambda_best_ridge
```

#### Lasso Regression
```{r}
#Set up Lasso Model
lasso_mod = cv.glmnet(train_x, train_y, alpha = 1)
#alpha = 1 is lasso regression (default) 
plot(lasso_mod)
lambda_best_lasso = lasso_mod$lambda.min
lambda_best_lasso

```
#### Principal Components Regression (PCR)
```{r}
#pls library
pcr_mod = pcr(crim ~ ., data = train, scale = TRUE, validation = "CV")
summary(pcr_mod)
validationplot(pcr_mod, val.type = "MSEP")
#We find that the lowest MSEP occurs when then number of components = 12
```
#### Partial Least Squares (PLS)
```{r}
pls_mod = plsr(crim ~ ., data = train, scale = TRUE, validation = "CV")
summary(pls_mod)
validationplot(pls_mod, val.type = "MSEP")
#We find that the lowest MSEP occurs when then number of components = 9
```

#### Least Squares (LS)
```{r}
#Least Squares
ls_mod = lm(crim ~ ., data = train)
summary(ls_mod)
```


### 6.6.11.b
Propose a model (or set of models) that seem to perform well on
this data set, and justify your answer. Make sure that you are
evaluating model performance using validation set error, crossvalidation,
or some other reasonable alternative, as opposed to
using training error.

**Answer**
```{r}
#Test error: MSE
#Best Subset Selection
#Function to get predictions from the regsubset function
predict_regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[, xvars] %*% coefi
}

test_error = rep(NA, 11)

for (i in 1:11){
  pred = predict_regsubsets(best_sub_mod, test, id = i)
  #store in array
  test_error[i] <-(mean((pred - test_y)^2))
}
test_error

plot(test_error, xlab = "Number of Predictors", ylab = "Test MSE", main = "Best Subset Selection Test Error", type = "b")
```

```{r}
#TEST MSE continued

#Ridge
ridge_pred = predict(ridge_mod, s = lambda_best_ridge, newx = test_x)
#newx should be the test set x
ridge_test_error = mean((ridge_pred - test_y)^2)
ridge_test_error

#Lasso
lasso_pred = predict(lasso_mod, s = lambda_best_lasso, newx = test_x)
lasso_test_error = mean((lasso_pred - test_y)^2)
lasso_test_error

#PCR
#since we found that the lowest MSEP occurs when then number of components = 12
pcr_pred = predict(pcr_mod, test_x, ncomp = 12)
mean((pcr_pred - test_y)^2)

#PLS
#since we found that the lowest MSEP occurs when then number of components = 9
pls_pred = predict(pls_mod, test_x, ncomp = 9)
mean((pls_pred - test_y)^2)

#Least Squares
ls_pred = predict(ls_mod, test)
mean((ls_pred - test_y)^2)
```
Looking at the Test MSE for each model, we find that the Ridge Regression model has the lowest Test MSE. Therefore, the Ridge Regression model seems to perform well on this data set. The RMSE will perform similarly since it is the square root of the MSE.

```{r}
#Test RMSE (square root of MSE)
#Least Squares
ls_test_rmse = sqrt(mean((ls_pred - test_y)^2))
ls_test_rmse

#Ridge
ridge_test_rmse = sqrt(mean((ridge_pred - test_y)^2))
ridge_test_rmse

#Lasso
lasso_test_rmse = sqrt(mean((lasso_pred - test_y)^2))
lasso_test_rmse

#PCR
pcr_test_rmse = sqrt(mean((pcr_pred - test_y)^2))
pcr_test_rmse

#PLS
pls_test_rmse = sqrt(mean((pls_pred - test_y)^2))
pls_test_rmse

```

Now using Cross Validation methods to find the best model
```{r}
#boot library 

#CV RMSE
#Least Squares

summary(ls_mod)
ls_cv_mod = glm(crim ~ ., data = train)
ls_cv <- cv.glm(train,ls_cv_mod, K = 10)
ls_cv_rmse <- sqrt(mean(ls_cv$delta^2))
ls_cv_rmse

#Ridge
ridge_cv_pred = predict(ridge_mod, s = lambda_best_ridge, newx = train_x) 
#newx should be train_x
ridge_cv_rmse = sqrt(mean((ridge_cv_pred - train_y)^2))
ridge_cv_rmse

#Lasso
lasso_cv_pred = predict(lasso_mod, s = lambda_best_lasso, newx = train_x) 
#newx should be train_x
lasso_cv_rmse = sqrt(mean((lasso_cv_pred - train_y)^2))
lasso_cv_rmse

#PCR
pcr_cv_pred = predict(pcr_mod, train_x, ncomp = 12)
pcr_cv_rmse = sqrt(mean((pcr_cv_pred - train_y)^2))
pcr_cv_rmse

#PLS
pls_cv_pred = predict(pls_mod, train_x, ncomp = 9)
pls_cv_rmse = sqrt(mean((pls_cv_pred - train_y)^2))
pls_cv_rmse
```
My chosen model is Lasso Regression because it has the lowest Test RMSE and the second lowest Cross Validation RMSE. Therefore, the Lasso Regression model seems to perform well on this data set.

### 6.6.11.c
Does your chosen model involve all of the features in the data
set? Why or why not?

```{r}
#Lasso Regression
coef(lasso_mod, s = lambda_best_lasso)
```

**Answer**
Regarding the number of features, the Lasso Regression uses all the features in the data set. though not all are considered important. Lasso involves a penalty term that can shrink coefficients to 0. As shown above, some of these coefficients are close to 0, implying that the corresponding features have little impact on the response variable. For example, age and tax have the lowest coefficients (0.013 and -0.002) do not seem to contribute much to the prediction. However, nox has a large coefficient of -9.656, suggesting that it has a significant impact on the predicted outcome. This makes it useful for feature selection, especially when dealing with high-dimensional data, as it can help to identify the most important features.

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

CODE:

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | 49.91849| 5.946654 | |
| Ridge | 6.873875 | 5.92277 | |
| Lasso | 6.866915 | 5.936433 | |
| PCR | 6.863679 | 5.946654 | |
| PLS | 6.863741 | 5.946256 | |

NOTE: The values above may differ from the values that output through the code. I am not sure why the values may differ when I render the code. I have tried to set the seed to 17 (I even tried putting extra ones to each chunk and running them) to ensure that the results are reproducible. It stays consistent in the code when I rerun the chunk, but not when I render the document. Below are the values I get in my rendered output.

RENDERED:

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | 52.70157| 6.111392 | |
| Ridge | 6.764253 | 6.112843 | |
| Lasso | 6.740723 | 6.096148 | |
| PCR | 6.801432 | 6.188783 | |
| PLS | 6.726659 | 6.107701 | |

*NOTE:I will use the values in the RENDERED document for the final results and analysis.

**Answer**
As shown in the rendered table above, the Lasso Regression model has the lowest Test RMSE (6.096148), but Partial Least Squares has the lowest CV RMSE (6.726659). Therefore, these two models seem to perform well, but the Lasso Regression model can perform the best to unseen data.

Overall, the test error is slightly lower than the cross validation error for most of the models. This suggests that the model is generalizing well to the test data. While this is a very rare phenomenon and it could be due to chance on the dataset chosen, I have tried to set the seed to different ones (1, 11, 212, etc) and the results are consistent.


## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$

**Answer**
$$
\begin{align*}
&\text{We know that } \hat{\beta} \text{ is the least squares estimate, so it minimizes the training error:} \\
&\frac{1}{N} \sum_{i=1}^N (y_i - \hat{\beta}^T x_i)^2 \leq \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2 \text{ for all } \beta \\
&\text{Taking the expectation of both sides:} \\
&\operatorname{E}\left[\frac{1}{N} \sum_{i=1}^N (y_i - \hat{\beta}^T x_i)^2\right] \leq \operatorname{E}\left[\frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2\right] \text{ for all } \beta \\
\end{align*}
$$
$$
\begin{align*}
E(R_{\text{train}}(\hat{\beta})) &= \frac{1}{N} \sum_{i=1}^N \operatorname{E}[(y_i - \hat{\beta}^T x_i)^2] \\
&= \frac{1}{N} \sum_{i=1}^N \operatorname{E}[(y_i - \hat{\beta}^T x_i)^2] \\
&= \operatorname{E}[(y_i - \hat{\beta}^T x_i)^2] \\
\end{align*}
$$
$$
\begin{align*}
E(R_{\text{test}}(\tilde{\beta})) &= \frac{1}{M} \sum_{i=1}^M \operatorname{E}[(y_i - \tilde{\beta}^T x_i)^2] \\
&= \frac{1}{M} \sum_{i=1}^M \operatorname{E}[(y_i - \tilde{\beta}^T x_i)^2] \\
&= \operatorname{E}[(y_i - \tilde{\beta}^T x_i)^2] \\
\end{align*}
$$
$$
\begin{align*}
&\text{Since } \hat{\beta} \text{ minimizes the training error, we have:} \\
\operatorname{E}[(y_i - \hat{\beta}^T x_i)^2] &\leq \operatorname{E}[(y_i - \tilde{\beta}^T x_i)^2] \text{ for all } \tilde{\beta} \\
&\text{Now, we know that } \hat{\beta} \text{ minimizes the training error, so it is the best estimate for the training data.} \\
&\text{However, the test data is drawn from the same population as the training data, so we have:} \\
\operatorname{E}[R_{\text{train}}(\hat{\beta})] &< \operatorname{E}[R_{\text{test}}(\hat{\beta})] \text{ for all } \beta \\
\end{align*}
$$
Thus, we have shown that $\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})]$.