---
title: "Biostat 212a Homework 2"
subtitle: "Due Feb 6, 2024 @ 11:59PM"
author: "YOUR NAME and UID"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---


## ISL Exercise 4.8.1 (10pts)
1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.

Note: 4.2 logistic function representation and 4.3 logit representation can be found on page 134 in ISL book.

$$
\text{Show that} \ p(X) = \frac{ e^{\beta_0 + \beta_1X}}{1 +{ e^{\beta_0 + \beta_1X} }} =
\text{is equivalent to} \ \left( \frac{p(X)}{1-p(X)} \right) = e^{\beta_0 + \beta_1X}
$$
Starting with the 4.2 logstic function representation we have:
$$
\begin{align*}
\ p(X) &= \frac{ e^{\beta_0 + \beta_1X}}{1 +{ e^{\beta_0 + \beta_1X} }} \\
\text{Let} \ X &= e^{\beta_0 + \beta_1X} \\
\text{Then} \ p(X) &= \frac{X}{1 +{X}} \\
p(X)(1 +X)  &= {X} &\text{ Multiply both sides by (1+X)} \\
p(X)(1) + p(X)(X) &= {X} &\text{ Distribute p(x)} \\
p(X) &= {X} -p(X)(X) &\text{ Subtract p(X)(X) from both sides} \\

p(X) &= {X}(1-p(X)) &\text{ Factor out X} \\
\frac{p(X)}{(1-p(X))} &= {X} &\text{ Divide both sides by 1-p(X)} \\
\frac{p(X)}{(1-p(X))} &= e^{\beta_0 + \beta_1} &\text{ Substitute back in X} \\

\end{align*}
$$
Thus, we have arrived at the 4.3 logit representation. We have shown that the logistic function representation and logit representation for the logistic regression model are equivalent.

## ISL Exercise 4.8.6 (10pts)
6. Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, βˆ0 = −6, βˆ1 = 0.05, βˆ2 = 1.

### 4.8.6 a.
(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

$$
\begin{align}
X_1 &= 40, X_2 =3.5 \\
\beta_0 &= -6, \beta_1 = 0.05, \beta_2 = 1 \\
p(X) &= \frac{ e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}{1 +{ e^{\beta_0 + \beta_1X_1 + \beta_2X_2} }} \\
p(X) &= \frac{ e^{-6 + (0.05*40) + (1*3.5)}}{1 +{ e^{-6 + (0.5*40) + (1*3.5)} }} \\
p(X) &= \frac{0.6065307}{1 + 0.6065307} \\
p(X) &= 0.3775407 \\
&= 0.38
\end{align}
$$
Note: ran in termnal:exp(-6 + (0.05*40) + (1*3.5)) -> 0.6065307 and 0.6065307/(1+0.6065307) -> 0.3775407

The probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class is about 38%.

### 4.8.6 b.
(b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?

$$
\begin{align}
p &= 0.5, X_1 = ? \\
\beta_0 &= -6, \beta_1 = 0.05, \beta_2 = 1 \\
\frac{p(X)}{1-p(X)} &= {e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}  \\
log(\frac{p(X)}{1-p(X)}) &= \beta_0 + \beta_1X_1 + \beta_2X_2 \\
log(\frac{0.5}{1-0.5}) &= -6 + (0.05*X_1) + (1*3.5) \\
log(1) &= -6 + (0.05*X_1) + (1*3.5) \\
0 &= -6 + (0.05*X_1) + (1*3.5) \\
6 &= 0.05*X_1 + 3.5 \\
2.5 &= 0.05*X_1 \\
X_1 &= 50 \\
\end{align}
$$
## ISL Exercise 4.8.9 (10pts)

9. This problem has to do with odds.

### 4.8.9 a.
(a) On average, what fraction of people with an odds of 0.37 of
defaulting on their credit card payment will in fact default?
$$
\begin{align}
\text{Odds} &= \frac{p}{1-p} = 0.37 \\
\text{Inverting this relationship: }
p &= \frac{0.37}{1+0.37} \\
&= \frac{0.37}{1.37} \\ 
&= 0.27
\end{align}
$$
On average, 27% of people with an odds of 0.37 of defaulting on their credit card payment will in fact default.

### 4.8.9 b.
(b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

$$
\begin{align}
p &= 0.16 \\
\text{Odds} &= \frac{0.16}{1-0.16} \\
&= \frac{0.16}{0.84} \\
&= 0.1904762 \\
&= 0.19
\end{align}
$$
The odds that she will default with 16% chance of defaulting her credit card is 0.19.

## ISL Exercise 4.8.13 (a)-(i) (50pts)
This question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

```{r}
library(ISLR2)
```


Link to documentation on Weekly dataset: <https://www.rdocumentation.org/packages/ISLR/versions/1.4/topics/Weekly>

### 4.8.13 a.
(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r}
#Numeric Summaries

#Summary
summary(Weekly)
#Structure
str(Weekly)
#Correlation Matrix
cor(Weekly[, -9]) 
#exclude the Direction column (9) because it is categorical and it will error that it needs to be numeric
```

```{r warnings = FALSE, message = FALSE}
#Graphical Summaries
library(GGally)

Weekly %>% ggpairs(progress = FALSE, #hide progress output
                   lower = list(continuous = wrap("points", alpha = 0.3, size=0.5)), diag=list(continuous='barDiag'), 
                   upper = list(continuous = wrap("cor", size = 2))) + theme(axis.text = element_text(size = 5), 
                  #change axis
                  strip.text.x = element_text(size = 5), #change row text label sizes
                  strip.text.y = element_text(size = 5)) #change col text label sizes
```


Patterns: 

There are no strong correlations between most of the variables. The scatter plots show no strong linear relationships between the variables, and all of the coefficients are very low (<0.1) except for the correlation between Year and Volume, which has a strong positive correlation coefficient of 0.841942. Most of the other correlation coefficients are negative, but really close to 0.

The bar plots show that the number of Up days is slightly more than the number of Down days, meaning that there were more times when the stock market was going up in a positive direction. 

The distribution of Volume is right-skewed.

### 4.8.13 b.
(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?


```{r}
#Logistic Regression
fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(fit)
```

Yes, Lag2 appears to be statistically significant as the p-value for Lag2 is 0.0296 which is less than 0.05.

### 4.8.13 c.
(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.


```{r}
#Generate probabilities for each observation using the fit model
probs <- predict(fit, type = "response")

#Create pred vector with "Down" as default
pred <- rep("Down", length(probs))
#Change pred to "Up" if the probability is greater than 0.5
pred[probs > 0.5] <- "Up"

#actual direction values of entire dataset
actual_direction <- Weekly$Direction

#Confusion Matrix - compare pred to actual Direction
table(pred, actual_direction)
#Overall Fraction of Correct Predictions
mean(pred == actual_direction)

length(probs) #total number (sum of all)
```


The confusion matrix allows us to see how well the logistic regression model performed in predicting the direction of the stock market using all of the predictors. In this case, it made 54 + 557 = 611 correct predictions and 48 + 480 = 528 incorrect predictions. The model made 48 Type I errors (false positives) and 480 Type II errors (false negatives). Thus, the model seems to be able to predict 'Up' correctly, but it fails to predict 'Down' correctly as it is only right about 11.15% of the time ((54/ (54+430) = 0.11). The overall fraction of correct predictions is 611/1089 = 0.56. 

### 4.8.13 d.
(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).


```{r}
#Lowest year is 1990 and highest year is 2010
min(Weekly$Year) #1990
max(Weekly$Year) #2010

##Create training and test sets
training_set <- Weekly[Weekly$Year < 2009, ] #since we know 1990 is the lowest year we can use this cutoff
test_set <- Weekly[Weekly$Year >= 2009, ] #2009-2010 data, since we know 2010 is the highest year

#Fit the model with Lad 2 as the only predictor
fit_lag2 <- glm(Direction ~ Lag2, data = training_set, family = binomial)
summary(fit_lag2)

#Predict - Generate probabilities for each observation using the fit model
probs_lag2 <- predict(fit_lag2, test_set, type = "response")
#Type = response because it's probability scaled 0 to 1 (default type is link which is log odds)

#Create pred vector with "Down" as default
pred_lag2 <- rep("Down", length(probs_lag2))
#Change pred to "Up" if the probability is greater than 0.5
pred_lag2[probs_lag2 > 0.5] <- "Up"

#actual Y direction values of test set
actual_Y = test_set$Direction
#Confusion Matrix - compare pred to actual Direction
table(pred_lag2, actual_Y)
#Overall Fraction of Correct Predictions
mean(pred_lag2 == actual_Y)

length(probs_lag2)
# ran in terminal: sum(9,5,34,56) = 104 to check

```

The model with only Lag2 as predictor performed slightly better relative to the model with all predictors. The model made 9 + 56 = 65 correct predictions, but also made 5 + 34 = 39 incorrect predictions. The model made 5 Type I errors (false positives) and 34 Type II errors (false negatives). The overall fraction of correct predictions is 65/104 = 0.625, which is a slight improvement from 0.56 (the previous model with all the predicotrs). 

### 4.8.13 e.
(e) Repeat (d) using LDA.

```{r}
#Load MASS library to use lda()
library(MASS)

#Fit the model with Lad 2 as the only predictor
fit_lag2_lda <- lda(Direction ~ Lag2, data = training_set)
#note: don't need to specify family = binomial because lda() is a classification model
fit_lag2_lda

#LDA is a classification model so we predict the class (need to specify $class)
pred_lag2_lda <- predict(fit_lag2_lda, test_set)$class
pred_lag2_lda

#Confusion Matrix - compare pred to actual Direction
table(pred_lag2_lda, test_set$Direction)
#Overall Fraction of Correct Predictions
mean(pred_lag2_lda == test_set$Direction)
```

The LDA model achieved similar results to the logistic regression model, with the same confusion matrix and overall fraction of correct predictions of 0.625

### 4.8.13 f.
(f) Repeat (d) using QDA.


```{r}
#Fit the model with Lad 2 as the only predictor
fit_lag2_qda <- qda(Direction ~ Lag2, data = training_set)
#note: don't need to specify family = binomial because qda() is a classification model
fit_lag2_qda

#Predict - Generate probabilities
pred_lag2_qda <- predict(fit_lag2_qda, test_set)$class

#Confusion Matrix - compare pred to actual Direction
table(pred_lag2_qda, test_set$Direction)

length(pred_lag2_qda)

```

The confusion matrix shows that the QDA model always chose "Up" as the prediction, and the overall fraction of correct predictions is 61/104= 0.5865. This implies that when the stock market was going down, the QDA model always predicted that it would go up and was right 0% of the time. When the stock market was going up it was right 100% of the time. Despite this, the QDA model performed only slightly worse than the LDA and logistic regression models (0.5865 vs 0.625).

### 4.8.13 g.
(g) Repeat (d) using KNN with K = 1.


```{r}
#Load class library to use knn()
library(class)

#Create training and test sets
train_X <- as.matrix(training_set$Lag2)
test_X <- as.matrix(test_set$Lag2)
train_Y_labels <- training_set$Direction
set.seed(1)
pred_lag2_knn <- knn(train = train_X, test = test_X, cl = train_Y_labels, k = 1)

#Model Evaluation
actual_Y <- test_set$Direction

#Confusion Matrix
table(pred_lag2_knn, actual_Y)

#Overall Fraction of Correct Predictions
mean(pred_lag2_knn == actual_Y)
```

With an overall fraction of correct predictions of 0.5, the KNN model with K=1 performed the worst out of all the models. The confusion matrix shows that the KNN model made 21+31= 52 correct predictions and 22+30 = 52 incorrect predictions. When the market is actually up, the KNN model predicted it correctly 31/(30+31) = 50.82% of the time. When the market is actually down, the KNN model predicted it correctly 21/(21+22) = 48.84% of the time. Perhaps a more optimal k is needed to improve the performance of the KNN model.

### 4.8.13 h.
(h) Repeat (d) using naive Bayes.

```{r}
#Load e1071 library to use naiveBayes()
library(e1071)

naiveBayes_lag2_fit <- naiveBayes(Direction ~ Lag2, data = training_set)
naiveBayes_lag2_pred <- predict(naiveBayes_lag2_fit, test_set)

table(naiveBayes_lag2_pred, actual_Y)
mean(naiveBayes_lag2_pred == actual_Y)
```

The naive Bayes model achieved similar results to the QDA model, obtaining the same confusion matrix and overall fraction of correct predictions of 0.58.

### 4.8.13 i.
(i) Which of these methods appears to provide the best results on this data?

The best performing models were Logistic Regression and LDA, with an overall fraction of correct predictions of 0.625. The QDA model and naive Bayes model performed slightly worse with an overall fraction of correct predictions of 0.5865. The KNN model with K=1 performed the worst with an overall fraction of correct predictions of 0.5.

## Bonus question: ISL Exercise 4.8.13 Part (j) (30pts)
(j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.


```{r}
#Logistic Regression with Lag4 and Lag5
fit_logistic_lag45 <- glm(Direction ~ Lag4:Lag5, data = training_set, family = binomial)
summary(fit_logistic_lag45)
probs_logistic_lag45 <- predict(fit_logistic_lag45, test_set, type = "response")
pred_logistic_lag45 <- rep("Down", length(probs_logistic_lag45))
pred_logistic_lag45[probs_logistic_lag45 > 0.5] <- "Up"
table(pred_logistic_lag45, actual_Y)
mean(pred_logistic_lag45 == actual_Y)
#0.5865385
```

```{r}
#Logistic Regression with first 3 Lags * Interaction Effect
fit_logistic_interaction <- glm(Direction ~ Lag1*Lag2*Lag3, data = training_set, family = binomial)
summary(fit_logistic_interaction)
probs_logistic_interaction <- predict(fit_logistic_interaction, test_set, type = "response")
pred_logistic_interaction <- rep("Down", length(probs_logistic_interaction))
pred_logistic_interaction[probs_logistic_interaction > 0.5] <- "Up"
table(pred_logistic_interaction, actual_Y)
mean(pred_logistic_interaction == actual_Y)
#0.6057692
```

```{r}
#Logistic Regression with first 2 Lags Interaction Effect and Log Transformation of Volume
fit_logistic_transformation <- glm(Direction ~ Lag1*Lag2 + log(Volume), data = training_set, family = binomial)
summary(fit_logistic_transformation)
probs_logistic_transformation <- predict(fit_logistic_transformation, test_set, type = "response")
pred_logistic_transformation <- rep("Down", length(probs_logistic_transformation))
pred_logistic_transformation[probs_logistic_transformation > 0.5] <- "Up"
table(pred_logistic_transformation, actual_Y)
mean(pred_logistic_transformation == actual_Y)
#0.6057692
```

```{r}
#LDA with combinations of odd lag numbers: Lag1, Lag3, and Lag5
fit_CombOdd_lda <- lda(Direction ~ Lag1+Lag3+Lag5, data = training_set)
#LDA is a classification model so we predict the class (need to specify $class)
pred_CombOdd_lda <- predict(fit_CombOdd_lda, test_set)$class
pred_CombOdd_lda

#Confusion Matrix - compare pred to actual Direction
table(pred_CombOdd_lda, test_set$Direction)
#Overall Fraction of Correct Predictions
mean(pred_CombOdd_lda == test_set$Direction)
#0.5384615
```

```{r}
#LDA with combinations of odd lag numbers: Lag1, Lag3, and Lag5
fit_CombOdd_lda <- lda(Direction ~ Lag1+Lag3+Lag5, data = training_set)
pred_CombOdd_lda <- predict(fit_CombOdd_lda, test_set)$class
pred_CombOdd_lda
table(pred_CombOdd_lda, actual_Y)
mean(pred_CombOdd_lda == actual_Y)
#0.5384615
```

```{r}
#LDA with sq and quadratic transformation and combination of even numbers Lag2 and Lag4
fit_lda_transformation <- lda(Direction ~ Lag2^2+Lag4^4, data = training_set)
pred_lda_transformation <- predict(fit_lda_transformation, test_set)$class
pred_lda_transformation
table(pred_lda_transformation, actual_Y)
mean(pred_lda_transformation == actual_Y)
#0.625 same as LDA Lag2
```

```{r}
#QDA with various transformations: squared, exp, and 1/x to Lag2:4
fit_qda_transformation <- qda(Direction ~ Lag2^2 + exp(Lag3) + (1/Lag4), data = training_set)
fit_qda_transformation
pred_qda_transformation <- predict(fit_qda_transformation, test_set)$class
table(pred_qda_transformation, actual_Y)
mean(pred_qda_transformation == actual_Y)
#0.4326923
```

```{r}
#QDA with combinations and interaction effect with Volume
fit_qda_interaction <- qda(Direction ~ Lag1*Volume + Lag2*Lag5*Volume, data = training_set)
fit_qda_interaction
pred_qda_interaction <- predict(fit_qda_interaction, test_set)$class
table(pred_qda_interaction, actual_Y)
mean(pred_qda_interaction == actual_Y)
#0.5192308
```

```{r}
#KNN with K=3

k_values <- 1:30
correction_accuracy_array <- c()
set.seed(212)
for (k in k_values) {
  pred_knn <- knn(train = train_X, test = test_X, cl = train_Y_labels, k = k)
  table(pred_knn, actual_Y)
  fraction_correct <- (mean(pred_knn == actual_Y))
  correction_accuracy_array <- c(correction_accuracy_array, fraction_correct) #append to array
}
print(correction_accuracy_array)

plot(k_values, correction_accuracy_array, type = "l", xlab = "K Values", ylab = "Overall Fraction of Correct Predictions")
title(main = "Exploring Different Values of K in K-Nearest Neighbors (KNN) \n for Performance of Correct Predictions")
```

```{r}
train_X <- as.matrix(training_set$Lag2)
test_X <- as.matrix(test_set$Lag2)
train_Y_labels <- training_set$Direction
set.seed(1)
pred_lag2_knn <- knn(train = train_X, test = test_X, cl = train_Y_labels, k = 1)

#Model Evaluation
actual_Y <- test_set$Direction

#Confusion Matrix
table(pred_lag2_knn, actual_Y)

#Overall Fraction of Correct Predictions
mean(pred_lag2_knn == actual_Y)
```

```{r}
#naive Bayes combinations (note: naive bayes can't handle interaction terms)
naiveBayes_fit_combination <- naiveBayes(Direction ~ Lag2+Lag5+Volume, data = training_set)
naiveBayes_pred_combination <- predict(naiveBayes_fit_combination, test_set)

table(naiveBayes_pred_combination, actual_Y)
mean(naiveBayes_pred_combination == actual_Y)
#0.4423077
```

```{r}
#naive Bayes cubic and squared transformation
naiveBayes_lag2_fit <- naiveBayes(Direction ~ Lag1^3 + Lag2^2 + Lag1, data = training_set)
naiveBayes_lag2_pred <- predict(naiveBayes_lag2_fit, test_set)

table(naiveBayes_lag2_pred, actual_Y)
mean(naiveBayes_lag2_pred == actual_Y)
#0.5384615
```



## Bonus question: ISL Exercise 4.8.4 (30pts)
When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test ob- servation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.

(a) Suppose that we have a set of observations, each with measure- ments on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test obser- vation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55,0.65]. On average, what fraction of the available observations will we use to make the prediction?

(b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3,0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?

(c) Now suppose that we have a set of observations on p = 100 fea- tures. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?

(d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training obser- vations “near” any given test observation.

(e) Now suppose that we wish to make a prediction for a test obser- vation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10 % of the train- ing observations. For p = 1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.
curse of di- mensionality

Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.
