---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Kathy Hoang and 506333118"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r}
#Clear Environment
rm(list=ls())
```


```{r}
#load libraries
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)
```

## ISL Exercise 9.7.1 (10pts)
This problem involves hyperplanes in two dimensions.
(a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set of
points for which 1 + 3X1 − X2 > 0, as well as the set of points
for which 1 + 3X1 − X2 < 0.

![9.7.1.a Hyperplane](images/9.7.1.a.jpg)

(b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0.
Indicate the set of points for which −2 + X1 + 2X2 > 0, as well
as the set of points for which −2 + X1 + 2X2 < 0.

![9.7.1.b Hyperplane](images/9.7.1.b.jpg)



## ISL Exercise 9.7.2 (10pts)
We have seen that in p = 2 dimensions, a linear decision boundary
takes the form $\beta_0 + \beta_1X_1 + \beta_2X_2 =0$.
We now investigate a non-linear decision boundary.

### a
(a) Sketch the curve $(1 + X_1)^2 + (2 − X_2)^2 = 4:$

![9.7.2.a Curve](images/9.7.2.a.jpg)

### b
(b) On your sketch, indicate the set of points for which

$(1 + X_1)^2 + (2 − X_2)^2 > 4;$

as well as the set of points for which
$(1 + X_1)^2 + (2 − X_2)^2 $\le$ 4:$

![9.7.2.b Set of Points](images/9.7.2.b.jpg)

### c
(c) Suppose that a classifier assigns an observation to the blue class
if
$(1 + X_1)^2 + (2 − X_2)^2 > 4;$
and to the red class otherwise. To what class is the observation
(0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?

![9.7.2.c Classifer for Observations](images/9.7.2.c.jpg)

### d
(d) Argue that while the decision boundary in (c) is not linear in
terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$,
and $X_2^2$.

![9.7.2.d Linear Decision Boundary](images/9.7.2.d.jpg)
**Written Explanation** 

The decision boundary is not linear in terms of $X_1$ and $X_2$ because the 
terms $X_1^2$ and $X_2^2$ make it quadratic. If it were linear, it would have 
the form $aX_1 + bX_2 + c = 0$, where a, b, and c are constants.

However, the decision boundary is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$ because they are represented in the form of a linear equation: $aX_1 + bX_1^2 + cX_2 + dX_2^2 + e = 0$, where a, b, c, d, and e are constants.

## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

```{r}
#load data
carseats_ds <- Carseats
carseats_ds |> tbl_summary()

#check for null
is.na(carseats_ds) |> colSums()

# Recode Sales as a Factor
carseats_ds$Sales <- factor(ifelse(carseats_ds$Sales > 8, "High", "Low"))
head(carseats_ds,5)
```

```{r}
#Split Data
set.seed(212)

carseats_split <- initial_split(
  carseats_ds, 
  prop = 0.75, 
  strata = Sales)
carseats_split
carseats_train <- training(carseats_split)
carseats_test <- testing(carseats_split)

dim(carseats_train)
dim(carseats_test)
```


```{r}
#Preprocess Data and Recipe
svm_carseats_recipe <- recipe(
  Sales ~ ., 
  data = carseats_train) |>
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

### SVM with linear kernel 

```{r}
svm_linear_mod <- 
  svm_linear(
    mode = "classification",
    cost = tune(),
    #degree = 1 #linear
  ) %>% 
  set_engine("kernlab")
svm_linear_mod

#Bundle
svm_linear_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_linear_mod)

#Tune
param_grid_linear <- grid_regular(
  cost(range = c(-2, 3)), #tune cost (C)
  #degree = 1, #bc linear
  levels = c(5)
  )
```
```{r}
#Cross-Validation
set.seed(212)

svm_linear_folds <- vfold_cv(carseats_train, v = 5)

#Fit C-V
svm_linear_fit <- svm_linear_wf %>%
  tune_grid(
    resamples = svm_linear_folds,
    grid = param_grid_linear,
    metrics = metric_set(roc_auc, accuracy)
    )

svm_linear_fit
```
```{r}
#Cost
svm_linear_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()
```
```{r}
#Best Model
svm_linear_fit %>%
  show_best("roc_auc")

best_svm_linear <- svm_linear_fit %>%
  select_best("roc_auc")

#Final Model
final_svm_linear <- svm_linear_wf %>%
  finalize_workflow(
    best_svm_linear
  )

#Fit Final Model
final_svm_lin_fit <- final_svm_linear %>%
  fit(data = carseats_train)
final_svm_lin_fit

#Predict
final_svm_preds <- final_svm_lin_fit %>%
  predict(new_data = carseats_test) %>%
  bind_cols(carseats_test)
final_svm_preds
```

### SVM with polynomial kernel 
Tune the degree and regularization parameter $C$.
```{r}
#Model
svm_mod <- 
  svm_poly(
    mode = "classification",
    cost = tune(),
    degree = tune(),
    # scale_factor = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod

#Bundle
svm_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_mod)
svm_wf

#Tune
param_grid <- grid_regular(
  cost(range = c(-2,3)), #tune cost (C) 
  #cost(range = c(-3,3)), #tune cost (C) 
  #on a logarithmic scale
  #10-3 to 10^3 (0.0001, 0.01, 0.1, 1, 100, 1000)
  degree(range = c(1, 5)), #tune degrees 1-5
  #scale_factor(range = c(-1, 1)),
  levels = c(5)
  )
param_grid

#Cross-Valisdation
set.seed(212)
svm_folds <- vfold_cv(carseats_train, v = 5)
svm_folds

#Fit C-V 
svm_fit <- svm_wf %>%
  tune_grid(
    resamples = svm_folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

```{r}
#Cost
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

#Degree
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = degree, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Degree", y = "CV AUC") +
  scale_x_log10()
```

```{r}
#Additional Plots - (better graphs mentioned in OH)

#Degree on x-axis, grouped by cost
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = degree, y = mean, alpha = cost)) +
  geom_point() +
  geom_line(aes(group = cost)) +
  labs(x = "Degree", y = "CV AUC") +
  scale_x_log10() 

#Cost on x-axis, grouped by degree
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = cost, y = mean, alpha = degree)) +
  geom_point() +
  geom_line(aes(group = degree)) +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10() 
```

```{r}
#Best Model
svm_fit %>%
  show_best("roc_auc")

best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm

#Final Model
final_svm <- svm_wf %>%
  finalize_workflow(
    best_svm
  )
final_svm

#Fit Final Model
final_svm_fit <- final_svm %>%
  fit(data = carseats_train)
final_svm_fit

#Predict
final_svm_preds <- final_svm_fit %>%
  predict(new_data = carseats_test) %>%
  bind_cols(carseats_test)
final_svm_preds
```

### SVM with radial kernel
```{r}
svm_radial_mod <- 
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>% 
  set_engine("kernlab")
```

```{r}
#Bundle
svm_radial_wf <- workflow() %>%
  add_recipe(svm_carseats_recipe) %>%
  add_model(svm_radial_mod)
```

```{r}
#Tune
param_grid_radial <- grid_regular(
  #cost(range = c(-2, 2)), #tune cost (C)
  cost(range = c(-3,5)), #tune cost (C)
  rbf_sigma(range = c(-4, -2)), #tune scale
  levels = c(5)
  )
```

```{r}
#Cross-Validation
set.seed(212)

svm_radial_folds <- vfold_cv(carseats_train, v = 5)

#Fit C-V
svm_radial_fit <- svm_radial_wf %>%
  tune_grid(
    resamples = svm_radial_folds,
    grid = param_grid_radial,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_radial_fit

svm_radial_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean, color = as.factor(rbf_sigma))) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

```
### HW4 Comparisons

**Answer**

Compared to the methods used in HW4, the SVM with linear kernel, SVM with 
polynomial kernel, and SVM with radial kernel seem to perform better overall 
with a higher AUC. The SVM with linear kernel (degree = 1) has the highest AUC 
above 0.96. The next highest AUC was achieved by the SVM with radial kernel,
with an AUC score over 0.95, which was reached by the rbf_sigma parameter of 
0.00316 (blue line). 

In HW4, the Regression Tree model's highest AUC was only slightly over 0.8, and 
the Random Forest model's highest AUC was just under 0.91. However, the Gradient Boosting had the highest AUC score of about 0.93, but was still under 0.95 so it performed worse than the SVMs above.

Sources:

Intuitive explanation on Tuning Cost and Gamma Parameters <https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py>

The degree parameter controls the flexibility of the decision boundary. Higher degree kernels yield a more flexible decision boundary. <https://stats.stackexchange.com/questions/348318/degree-parameter-for-svm-polynomial-kernel>

## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 

$$
\begin{aligned}
&f(X) = \beta_0 + \beta^T X \\

\\
&\text{1.Subtract from both sides: } \beta_0 \\
&f(X) = \beta_0 + \beta^T X \\
&f(X) - \beta_0 = \beta^T X \\
\\

&\text{2. For any X_0 in the hyperplane f(X_0) =0} \\
&f(X_0) -\beta_0 = \beta^T X_0 \\
\\

&\text{3. Set f(x) = 0} \\
&0 -\beta_0 = \beta^T X_0 \\
&-\beta_0 = \beta^T X_0 \\
\\

&\text{The signed distance of any point x from the hyperplane f(x)=0 is given 
by} \\
&\frac{1}{||\beta||}(\beta^Tx - \beta^Tx_0) \\
&= \frac{1}{||\beta||}(\beta^Tx + x_0) \\
&= \frac{1}{||\beta||}(f(x)) \\

&\text{where} \\
&||\beta|| = \sqrt{\sum_{j=1}^{p} \beta_j^2}\\
\\

&\text{Therefore, f(x) is proportional to the signed distance of a point x to 
the hyperplane f(X) = 0.} \\

\end{aligned}
$$